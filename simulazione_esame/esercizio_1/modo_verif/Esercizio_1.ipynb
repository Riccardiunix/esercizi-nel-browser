{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a334a19",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hide_input": true,
    "init_cell": true,
    "tags": [
     "noexport"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import IFrame, display, HTML, Markdown, clear_output, Javascript\n",
    "import ipywidgets as widgets\n",
    "\n",
    "def on_change(change): # si attiva quando cambio scelta dell'elemento da selezionare\n",
    "    global attach\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "        #print (\"Stai scegliendo %s\" % change['new'])\n",
    "        attach = change['new']\n",
    "        return attach\n",
    "\n",
    "def on_button_update(_):\n",
    "    clear_output()\n",
    "    #display(Javascript('''var c = IPython.notebook.get_selected_index();\n",
    "                       #IPython.notebook.execute_cells([c])'''))\n",
    "    allegato()\n",
    "\n",
    "def open_attachment(attach_name):\n",
    "    path_attachments = \"allegati\"\n",
    "    path_file=str(path_attachments + \"/\" + attach_name)\n",
    "    ext = os.path.splitext(path_file)[-1].lower()\n",
    "    #print(path_file)\n",
    "    #print(ext)\n",
    "    #print(ext.replace('.', ''))\n",
    "    if(ext=='.pdf'):\n",
    "        pdf=IFrame(path_file, width=700, height=400)\n",
    "        display(pdf)\n",
    "    else:\n",
    "        #file = open(path_file, \"rb\")\n",
    "        display(Markdown(\"![alt text](\" + path_file + \")\"))\n",
    "        delete = widgets.Button(description='Rimuovi allegato')\n",
    "        delete.on_click(on_button_delete)\n",
    "        display(delete)\n",
    "\n",
    "        #image = file.read()\n",
    "        #w=widgets.Image(\n",
    "        #    value=image,\n",
    "        #    format=ext.replace('.', ''),\n",
    "        #    width=300,\n",
    "        #    height=400,\n",
    "        #)\n",
    "        #display(w)\n",
    "\n",
    "def on_button_confirm(_):\n",
    "    global attach\n",
    "    clear_output()\n",
    "    print('Allegato: ' + str(attach))\n",
    "    open_attachment(attach)\n",
    "\n",
    "def allegato():\n",
    "    path_attachments = \"allegati\"\n",
    "    global attach # allegato attuale scelto\n",
    "    update = widgets.Button(description='Aggiorna') #bottone visualizzato in ogni caso\n",
    "    delete = widgets.Button(description='Rimuovi allegato')\n",
    "    attach_list = os.listdir(path_attachments)\n",
    "    if 'ck_points' in attach_list:\n",
    "        attach_list.remove('ck_points')\n",
    "\n",
    "    if(attach_list): #si attiva se c'è almeno un elemento in lista quindi nella cartella allegati\n",
    "        confirm = widgets.Button(description='Conferma')\n",
    "        chosen_attach_wid = widgets.Dropdown(\n",
    "            options = attach_list,\n",
    "            value = attach_list[0],\n",
    "            description = 'Allegati:',\n",
    "            disabled = False,\n",
    "        )\n",
    "        attach = attach_list[0]\n",
    "        chosen_attach_wid.observe(on_change)\n",
    "        update.on_click(on_button_update)\n",
    "        confirm.on_click(on_button_confirm)\n",
    "        delete.on_click(on_button_delete)\n",
    "        all_buttons = [update, chosen_attach_wid, confirm, delete]\n",
    "        display(widgets.HBox(all_buttons))\n",
    "    else:\n",
    "        print(f\"ATTENZIONE: non sono presenti file nella cartella allegati.\") #Ti ricordiamo che la cartella utile per gli allegati da te prodotti per questo esercizio in modo_libero di svolgimento è la cartella @path_ex_folder@/modo_libero/allegati/\")\n",
    "        update.on_click(on_button_update)\n",
    "        delete.on_click(on_button_delete)\n",
    "        all_buttons = [update, delete]\n",
    "        display(widgets.HBox(all_buttons))\n",
    "\n",
    "def on_button_delete(_):\n",
    "    #delete_above_cell()\n",
    "    delete_this_cell()\n",
    "\n",
    "def delete_above_cell():\n",
    "    display(Javascript('''var c = IPython.notebook.get_selected_index();\n",
    "                                IPython.notebook.get_cell(c-1).metadata.editable = true;\n",
    "                                IPython.notebook.get_cell(c-1).metadata.deletable = true;\n",
    "                                IPython.notebook.delete_cell([c-1]);'''))\n",
    "def delete_this_cell():\n",
    "    display(Javascript('''var c = IPython.notebook.get_selected_index();\n",
    "                                IPython.notebook.get_cell(c).metadata.editable = true;\n",
    "                                IPython.notebook.get_cell(c).metadata.deletable = true;\n",
    "                                IPython.notebook.delete_cell([c]);'''))\n",
    "\n",
    "def on_button_raw_attach(_):\n",
    "    add_attach_cell_din('allegato()')\n",
    "\n",
    "def on_button_md(_):\n",
    "    add_md_cell_din()\n",
    "\n",
    "def on_button_code(_):\n",
    "    add_code_cell_din()\n",
    "\n",
    "def on_button_raw(_):\n",
    "    add_raw_cell_din()\n",
    "\n",
    "def add_raw_cell_din():\n",
    "    display_id = int(time.time()*1e9) # Hack\n",
    "    display(Javascript('''var idx = IPython.notebook.get_selected_index();\n",
    "                       var c = IPython.notebook.insert_cell_at_index(\"raw\", idx);\n",
    "    c.set_text('');\n",
    "    var t_index = IPython.notebook.get_cells().indexOf(c);\n",
    "    IPython.notebook.to_raw(t_index);\n",
    "    IPython.notebook.get_cell(t_index).render();\n",
    "    IPython.notebook.get_cell(t_index).metadata.deletable = true;\n",
    "    IPython.notebook.get_cell(t_index).set_text('');;'''),display_id=display_id) # Hack\n",
    "    # Necessary hack to avoid self-generation of cells at notebook re-opening\n",
    "    # See http://tiny.cc/fnf3nz\n",
    "    display(Javascript(\"\"\" \"\"\"), display_id=display_id, update=True)\n",
    "    return\n",
    "\n",
    "def add_attach_cell_din(code = ''):\n",
    "    display_id = int(time.time()*1e9) # Hack\n",
    "    display(Javascript('''var c = IPython.notebook.insert_cell_above();\n",
    "    c.set_text(' ''' + code + ''' ');\n",
    "    var t_index = IPython.notebook.get_cells().indexOf(c);\n",
    "    IPython.notebook.to_code(t_index);\n",
    "    IPython.notebook.get_cell(t_index).render();\n",
    "    IPython.notebook.execute_cells([t_index]);\n",
    "    IPython.notebook.get_cell(t_index).metadata.editable = false;'''),display_id=display_id) # Hack\n",
    "    # Necessary hack to avoid self-generation of cells at notebook re-opening\n",
    "    # See http://tiny.cc/fnf3nz\n",
    "    display(Javascript(\"\"\" \"\"\"), display_id=display_id, update=True)\n",
    "    return\n",
    "\n",
    "def add_code_cell_din(code = ''):\n",
    "    display_id = int(time.time()*1e9)\n",
    "    display(Javascript('''var c = IPython.notebook.insert_cell_above();\n",
    "    c.set_text(' ''' + code + ''' ');\n",
    "    var t_index = IPython.notebook.get_cells().indexOf(c);\n",
    "    IPython.notebook.to_code(t_index);\n",
    "    IPython.notebook.get_cell(t_index).render();\n",
    "    IPython.notebook.execute_cells([t_index]);\n",
    "    IPython.notebook.get_cell(t_index).metadata.deletable = true;\n",
    "    IPython.notebook.get_cell(t_index).set_text('');'''),display_id=display_id)\n",
    "    # Necessary hack to avoid self-generation of cells at notebook re-opening\n",
    "    # See http://tiny.cc/fnf3nz\n",
    "    display(Javascript(\"\"\" \"\"\"), display_id=display_id, update=True)\n",
    "    return\n",
    "\n",
    "def add_md_cell_din():\n",
    "    display_id = int(time.time()*1e9)\n",
    "    display(Javascript('''var c = IPython.notebook.insert_cell_above();\n",
    "    c.set_text(' ');\n",
    "    var t_index = IPython.notebook.get_cells().indexOf(c);\n",
    "    IPython.notebook.to_markdown(t_index);\n",
    "    IPython.notebook.get_cell(t_index).render();\n",
    "    IPython.notebook.get_cell(t_index).metadata.deletable = true;\n",
    "    IPython.notebook.get_cell(t_index).set_text('');'''),display_id=display_id) # Hack\n",
    "    # Necessary hack to avoid self-generation of cells at notebook re-opening\n",
    "    # See http://tiny.cc/fnf3nz\n",
    "    display(Javascript(\"\"\" \"\"\"), display_id=display_id, update=True)\n",
    "    return\n",
    "\n",
    "def add_cell(code='', position='below', celltype='markdown', is_execute = False):\n",
    "    \"\"\"Create a cell in the IPython Notebook.\n",
    "    code: unicode, Code to fill the new cell with.\n",
    "    celltype: unicode, Type of cells \"code\" or \"markdown\".\n",
    "    position: unicode, Where to put the cell \"below\" or \"at_bottom\"\n",
    "    is_execute: boolean, To decide if the cell is executed after creation\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a unique id based on epoch time\n",
    "    display_id = int(time.time()*1e9)\n",
    "\n",
    "    if is_execute:\n",
    "        display(Javascript(\"\"\"\n",
    "        var basis = IPython.notebook.insert_cell_{0}(\"{1}\");\n",
    "        basis.set_text(atob(\"{2}\"));\n",
    "        basis.execute();\n",
    "        \"\"\".format(position, celltype, \" \")),display_id=display_id)\n",
    "\n",
    "    else:\n",
    "        display(Javascript(\"\"\"\n",
    "        var basis = IPython.notebook.insert_cell_{0}(\"{1}\");\n",
    "        basis.set_text(atob(\"{2}\"));\n",
    "        \"\"\".format(position, celltype, \" \")),display_id=display_id)\n",
    "\n",
    "\n",
    "    # Necessary hack to avoid self-generation of cells at notebook re-opening\n",
    "    # See http://tiny.cc/fnf3nz\n",
    "    display(Javascript(\"\"\" \"\"\"), display_id=display_id, update=True)\n",
    "\n",
    "def code_button_delete():\n",
    "    button_delete = widgets.Button(description=\"Rimuovi l'allegato\", tooltip=\"Seleziona la cella e clicca su Elimina\")\n",
    "    button_delete.on_click(on_button_delete)\n",
    "    #print(\"Vuoi eliminare la cella selezionata?\")\n",
    "    display(button_delete)\n",
    "\n",
    "def loader_main():\n",
    "    button_raw_attach = widgets.Button(description=\"(Cella per Allegato)\", tooltip=\"Collega un file dalla cartella attachments\")\n",
    "    button_md = widgets.Button(description=\"(Cella Markdown)\", abstooltip=\"Aggiungi una cella per scrivere del testo in Markdown\")\n",
    "    button_code = widgets.Button(description=\"(Cella Codice)\", tooltip=\"Aggiungi una cella per scrivere del codice in Python\")\n",
    "    button_raw = widgets.Button(description=\"(Cella Testo ASCII)\", tooltip=\"Aggiungi una cella per scrivere del testo libero (SCONSIGLIATA: righe troppo lunghe potrebbero fuoriuscire nella rendition, usala solo se non riesci a controllare il Markdown)\")\n",
    "    button_raw_attach.on_click(on_button_raw_attach)\n",
    "    button_md.on_click(on_button_md)\n",
    "    button_code.on_click(on_button_code)\n",
    "    button_raw.on_click(on_button_raw)\n",
    "\n",
    "    all_buttons = [button_code, button_md, button_raw, button_raw_attach]\n",
    "    display(widgets.HBox(all_buttons))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43d3583",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hide_input": true,
    "init_cell": true,
    "tags": []
   },
   "source": [
    "# ESERCIZIO: Problema dello zaino #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbccbb2e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hide_input": true,
    "init_cell": true,
    "tags": [
     "noexport"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import display, HTML, Markdown, clear_output, Javascript\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import IFrame\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a442147b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hide_input": true,
    "init_cell": true,
    "tags": [
     "noexport"
    ]
   },
   "outputs": [],
   "source": [
    "    from tabulate import tabulate\n",
    "    import copy\n",
    "    n_tasks = 10;\n",
    "    arr_point = [-1] * n_tasks;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b42454",
   "metadata": {
    "deletable": false,
    "editable": true,
    "hide_input": false,
    "init_cell": true,
    "tags": [
     "noexport"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "    labels= ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'L', 'M', 'N']\n",
    "    costs = [15, 16, 17, 11, 13, 5, 7, 3, 1, 12, 9, 7]\n",
    "    vals = [50, 52, 54, 40, 45, 17, 18, 7, 8, 42, 30, 22]\n",
    "    CapacityMax= 36\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376eea1d",
   "metadata": {
    "deletable": false,
    "editable": true,
    "hide_input": false,
    "init_cell": true,
    "tags": [
     "noexport"
    ]
   },
   "outputs": [],
   "source": [
    "# RTal python library for Jupyter Notebook usage\n",
    "# Authors: Michele Albrigo, Romeo Rizzi\n",
    "# Requirements: websocket, json\n",
    "\n",
    "# to install websocket inside a Jupyter notebook use:\n",
    "# import sys\n",
    "# !{sys.executable} -m pip install websocket-client\n",
    "\n",
    "from sys import stderr\n",
    "import websocket\n",
    "from IPython.display import display, HTML\n",
    "import json\n",
    "import ruamel.yaml\n",
    "import os\n",
    "\n",
    "RTAL_PRIVATE_ACCESS_TOKEN=\"id123456_VR123456_gCj0c44poMqbW5x\"\n",
    "PATH_EDITABLE_SETTINGS = os.getcwd()\n",
    "while os.path.isdir(PATH_EDITABLE_SETTINGS):\n",
    "    if os.path.isfile(os.path.join(PATH_EDITABLE_SETTINGS,'settings','settings.yaml')):\n",
    "        break\n",
    "    PATH_EDITABLE_SETTINGS = os.path.join(PATH_EDITABLE_SETTINGS,'..')\n",
    "    SETTINGS_FILE_FULLNAME=os.path.join(PATH_EDITABLE_SETTINGS,'settings','settings.yaml') \n",
    "with open(SETTINGS_FILE_FULLNAME,\"r\") as stream:\n",
    "     settings = ruamel.yaml.safe_load(stream)\n",
    "\n",
    "\n",
    "def info():\n",
    "    print(\"These are your current dynamic settings, as set in the file\",end=\"\\n   \")\n",
    "    print(SETTINGS_FILE_FULLNAME)\n",
    "    for k,v in settings.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "    hint = \"\"\"\n",
    "To see all the info you can get enter and run this piece of code:\n",
    "   vars = [k for k in locals().keys() if k[0] != '_']\n",
    "   vars.sort()\n",
    "   print('\\\\n'.join(iter(vars)))\"\"\"\n",
    "    print(hint)\n",
    "\n",
    "def printif(category_of_interest,msg):\n",
    "    if category_of_interest in settings['RTAL_INTERFACE_VERBOSITY']:\n",
    "        print(msg)\n",
    "\n",
    "    \n",
    "def rtal_list(rtal_URL):\n",
    "    \"\"\"lists all problems served by the rtald server\n",
    "    Parameters:\n",
    "    - rtal_URL: rtal server address\"\"\"\n",
    "    try:\n",
    "        rtalws = websocket.create_connection(rtal_URL)\n",
    "        rtalws.send('{\"Handshake\":{\"magic\":\"rtal\",\"version\":2}}')\n",
    "        printif('TALight_protocol','TALight return code:' + rtalws.recv())\n",
    "        rtalws.send('{\"MetaList\":{}}')\n",
    "        listjson = json.loads(rtalws.recv())\n",
    "        printif('TALight_protocol',list((listjson['MetaList'])['meta'].keys()))\n",
    "        rtalws.close()\n",
    "    except ConnectionRefusedError:\n",
    "        printif('TALight_protocol_signals_problem',f'Could not connect to RTAL server {rtal_URL}')\n",
    "\n",
    "def rtal_connect(rtal_URL, rtalproblem, rtalservice, rtalargs_dict, rtaltoken=None, output_files_local_folder=None):\n",
    "    \"\"\"sends a TALight connect request (problem, service, args)\n",
    "    Parameters:\n",
    "    - rtal_URL: rtal server address\n",
    "    - rtalproblem: name of the TALight problem offering the solution checking and oracle services\n",
    "    - rtalservice: name of the checking TALight service of interest\n",
    "    - rtalargs_dict: dictionary with the arguments to be sent to the TALight service\n",
    "    - output_files_local_folder (optional): the name of the local folder where TALight could store files like e.g. a certificate for a good submission received \n",
    "    - rtaltoken (optional): the student access token so that logs are stored on the server or even just to access the service\"\"\"\n",
    "    rtalfiles= 'RO_knapsack_out.txt'\n",
    "    \"\"\"NOTA [MISTERO DELLA FEDE]: se ora da quì dentro faccio un qualsiasi print (anche se su stderr) prima di entrare nel try e farlo come da lì, allora le cose si rompono e chi ha chiamato la funzione lamenta di non aver ricevuto risposta attribuendolo ad invalid TOKEN.\n",
    "       Ad esempio se faccio:\n",
    "           print(\"\\n\\n\\n\" f\"rtalargs_dict={rtalargs_dict}\" \"\\n\\n\\n\")\n",
    "       Poi successivamente nelle richieste nel notebook esprime:\n",
    "           tmp_first_reply={'ConnectBegin': {'status': {'Err': 'Invalid token'}}}\n",
    "Ho incontrato errori nel richiedere a TALight il servizio check del problema RO_knapsack\n",
    "Causa Errore: Invalid token\n",
    "Spiacenti, il feedback da TALight non ha potuto essere prodotto.\n",
    "       e la situazione non si ripristina a corretta nemmeno se reverto il cambiamento, il suo effetto resta permanente anche se rieseguo tutte le celle.\n",
    "    \"\"\"\n",
    "    args_string = \"\"\n",
    "    for key in rtalargs_dict:\n",
    "        args_string += f'\"{key}\":\"{rtalargs_dict[key]}\", '\n",
    "    args_string = args_string[:-2]\n",
    "    try:\n",
    "        rtalws = websocket.create_connection(rtal_URL)\n",
    "        rtalws.send('{\"Handshake\":{\"magic\":\"rtal\",\"version\":2}}')\n",
    "        connection_request_outcome = rtalws.recv()\n",
    "        printif('TALight_protocol','TALight return code:' + connection_request_outcome)\n",
    "        if rtaltoken == None:\n",
    "            if output_files_local_folder == None:\n",
    "                printif('TALight_protocol',\"sending 1: \" + '{\"ConnectBegin\":{\"problem\":\"'+rtalproblem+'\",\"service\":\"'+rtalservice+'\",\"args\":{'+args_string+'},\"tty\":false,\"files\":[\"'+rtalfiles+'\"]}}')\n",
    "                rtalws.send('{\"ConnectBegin\":{\"problem\":\"'+rtalproblem+'\",\"service\":\"'+rtalservice+'\",\"args\":{'+args_string+'},\"tty\":false,\"files\":[\"'+rtalfiles+'\"]}}')\n",
    "            else:\n",
    "                printif('TALight_protocol',\"sending 2: \" + '{\"ConnectBegin\":{\"problem\":\"'+rtalproblem+'\",\"service\":\"'+rtalservice+'\",\"args\":{'+args_string+'},\"tty\":false,\"files\":[\"'+rtalfiles+'\"]}}')\n",
    "                rtalws.send('{\"ConnectBegin\":{\"problem\":\"'+rtalproblem+'\",\"service\":\"'+rtalservice+'\",\"args\":{'+args_string+'},\"tty\":false,\"files\":[\"'+rtalfiles+'\"]}}')\n",
    "        else:\n",
    "            if output_files_local_folder == None:\n",
    "                printif('TALight_protocol',\"sending 3: \" + '{\"ConnectBegin\":{\"problem\":\"'+rtalproblem+'\",\"service\":\"'+rtalservice+'\",\"args\":{'+args_string+'},\"tty\":false,\"token\":\"'+rtaltoken+'\",\"files\":[\"'+rtalfiles+'\"]}}')\n",
    "                rtalws.send('{\"ConnectBegin\":{\"problem\":\"'+rtalproblem+'\",\"service\":\"'+rtalservice+'\",\"args\":{'+args_string+'},\"tty\":false,\"token\":\"'+rtaltoken+'\",\"files\":[\"'+rtalfiles+'\"]}}')\n",
    "            else:\n",
    "                printif('TALight_protocol',\"sending 4: \" + '{\"ConnectBegin\":{\"problem\":\"'+rtalproblem+'\",\"service\":\"'+rtalservice+'\",\"args\":{'+args_string+'},\"tty\":false,\"token\":\"'+rtaltoken+'\",\"files\":[\"'+rtalfiles+'\"]}}')\n",
    "                rtalws.send('{\"ConnectBegin\":{\"problem\":\"'+rtalproblem+'\",\"service\":\"'+rtalservice+'\",\"args\":{'+args_string+'},\"tty\":false,\"token\":\"'+rtaltoken+'\",\"files\":[\"'+rtalfiles+'\"]}}')\n",
    "        #tmp_first_reply = ruamel.yaml.safe_load(rtalws.recv())\n",
    "        tmp_first_reply = json.loads(rtalws.recv())\n",
    "        printif('TALight_protocol',f'tmp_first_reply={tmp_first_reply}')\n",
    "        if \"Err\" in tmp_first_reply[\"ConnectBegin\"][\"status\"]:\n",
    "            printif('TALight_protocol_signals_problem',f'Ho incontrato errori nel richiedere a TALight il servizio {rtalservice} del problema {rtalproblem}')\n",
    "            printif('TALight_protocol_signals_problem',\"Causa Errore: \" + tmp_first_reply[\"ConnectBegin\"][\"status\"][\"Err\"])\n",
    "            printif('TALight_protocol_signals_problem',\"Spiacenti, il feedback da TALight non ha potuto essere prodotto.\")\n",
    "            return None\n",
    "        else:\n",
    "            connection_request_outcome = rtalws.recv()\n",
    "            printif('TALight_protocol','TALight connection status:' + connection_request_outcome)\n",
    "            TAL_service_printout = rtalws.recv().decode('UTF-8')\n",
    "            printif('TALight_protocol','TALight service printout:' + TAL_service_printout)\n",
    "            feedback_dict={'feedback_string':TAL_service_printout}\n",
    "            if '\"as_yaml_with_points\":\"1\"' in args_string:\n",
    "                try:\n",
    "                    feedback_dict=eval(TAL_service_printout)\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "            if '\"color_implementation\":\"html\"' in args_string:\n",
    "                feedback_dict['feedback_string']=HTML(feedback_dict['feedback_string'])\n",
    "            return feedback_dict\n",
    "            if output_files_local_folder != None:\n",
    "               with open(output_files_local_folder, 'w') as output_file:\n",
    "                    output_file.write(TAL_service_printout)\n",
    "                    output_file.close()\n",
    "            rtalws.close()\n",
    "    except ConnectionRefusedError:\n",
    "        printif('TALight_protocol_signals_problem',f'Could not connect to RTAL server {rtal_URL}')\n",
    "        \n",
    "\n",
    "\n",
    "# Test TALight list command (check which problems are currently served. The purpouse is just to check if you reach the server):\n",
    "\n",
    "if settings['DEBUG_RTAL_AT_SET_UP']:\n",
    "    for rtal_URL_name in settings['RTAL_URLS']:\n",
    "        URL = settings[rtal_URL_name]\n",
    "        print(\"\\n\"f\"List of TAL problems served on server {rtal_URL_name} [{URL}]:\")\n",
    "        rtal_list(URL)\n",
    "\n",
    "# Example on how to lay down the data to prepare for an rtal_connect call for a specific problem when testing its correct interface as for the TALight server:\n",
    "\n",
    "task_dict={'task': 1, 'pt_tot': 40, 'pt_formato_OK': 0, 'pt_feasibility_OK': 1, 'pt_consistency_OK': 0, 'with_positive_enforcement':'1', 'with_notes':'1', 'as_yaml_with_points':'1','color_implementation':'html','with_output_files':'0'}\n",
    "input_data_assigned={'labels': ['A', 'B', 'C', 'D', 'E'], 'costs': [2, 3, 4, 5, 6], 'vals': [13, 17, 19, 30, 101], 'Knapsack_Capacity': 5, 'forced_out': [], 'forced_in': [], 'partialDPtable': []}\n",
    "alias_dict={'opt_sol69':'opt_sol','opt_val69':'opt_val'}\n",
    "answer_dict={'opt_sol69':['A','B'],'opt_val69':30}\n",
    "rtalargs_dict = task_dict\n",
    "rtalargs_dict['input_data_assigned'] = input_data_assigned\n",
    "rtalargs_dict['alias_dict'] = alias_dict\n",
    "rtalargs_dict['answer_dict'] = answer_dict\n",
    "\n",
    "def monitor_what_submitted_to_rtald(problem_name, rtalargs_dict):\n",
    "    if settings['MONITOR_CALLS_TO_RTAL_CHECKERS']:\n",
    "        command_line = f\"rtal connect {problem_name} check  \"\n",
    "        for key in rtalargs_dict:\n",
    "            command_line += f'-a {key}=\"{rtalargs_dict[key]}\"  '\n",
    "        print(\"\\n\\n\\n\" + \"*-------\"*5)\n",
    "        print(\"SICCOME PARLIAMO DIRETTAMENTE AD RTALD DOBBIAMO PASSARGLI LA LISTA COMPLETA DEGLI ARGOMENTI (IL DEFAULTING NON RISOLVE). QUESTO MESSAGGIO SERVE PER TENERTI CONSAPEVOLE DI COSA ESCE E CONSENTIRTI DI REPLICARLO AGEVOLMENTE A RIGA DI COMANDO PER TESTARLO IN CASO DI PROBLEMI\\n\\n\" f\"Gestire rtalargs_dict={rtalargs_dict}\" \"\\n\\ncorrisponde alla riga seguente di comando:\\n   \" f\"$ {command_line}\")\n",
    "        print(\"*-------\"*5 + \"*\\n\\n\\n\")\n",
    "\n",
    "\n",
    "monitor_what_submitted_to_rtald(\"RO_knapsack\", rtalargs_dict)\n",
    "\n",
    "# Example call with TOKEN to an rtal problem service meant to play as a checker:\n",
    "if settings['DEBUG_RTAL_AT_SET_UP']:\n",
    "    for rtal_URL_name in settings['RTAL_URLS']:\n",
    "        rtal_URL = settings[rtal_URL_name]\n",
    "        print(\"\\n\"f\"Now calling server {rtal_URL_name} [{rtal_URL}]:\")\n",
    "        feedback_dict = rtal_connect(rtal_URL, 'RO_knapsack', 'check', rtalargs_dict=rtalargs_dict, output_files_local_folder='output_files_TALight', rtaltoken=RTAL_PRIVATE_ACCESS_TOKEN)\n",
    "        print(\"\\n\"f\"Feedback from server {rtal_URL_name} [{rtal_URL}]:\")\n",
    "        if feedback_dict != None:\n",
    "            display(feedback_dict['feedback_string'])\n",
    "        else:\n",
    "            print(\"\\n\"f\"The Feedback received from the server {rtal_URL_name} is EMPTY\")\n",
    "            \n",
    "\n",
    "# Example call without TOKEN to an rtal problem service meant to play as a checker:\n",
    "if settings['DEBUG_RTAL_AT_SET_UP']:\n",
    "    for rtal_URL_name in settings['RTAL_URLS']:\n",
    "        rtal_URL = settings[rtal_URL_name]\n",
    "        feedback_dict = rtal_connect(rtal_URL, 'RO_knapsack', 'check', rtalargs_dict=rtalargs_dict, output_files_local_folder='output_files_TALight', rtaltoken=None)\n",
    "        print(f\"Feedback from server {rtal_URL_name} [{rtal_URL}]:\")\n",
    "        if feedback_dict != None:\n",
    "            display(feedback_dict['feedback_string'])\n",
    "        else:\n",
    "            print(\"The Feedback received from the server is EMPTY\")\n",
    "\n",
    "\n",
    "# Example call with to an rtal problem checker service with files saved in local:\n",
    "if settings['DEBUG_RTAL_AT_SET_UP']:\n",
    "    print(\"files saved in local: -NOT YET IMPLEMENTED-\")\n",
    "\n",
    "# Example call to an rtal problem service meant to play as an oracle:\n",
    "if settings['DEBUG_RTAL_AT_SET_UP']:\n",
    "    print(\"files saved in local: -NOT YET COME INTO THE GAME-\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6911f1",
   "metadata": {
    "deletable": false,
    "editable": true,
    "hide_input": false,
    "init_cell": true,
    "tags": [
     "noexport"
    ]
   },
   "outputs": [],
   "source": [
    "# General Instantiation for the Verifiers# library for the standardization of the feedback coming from the checkers placed within the jupyter notebook\n",
    "# it offers the same interface as a twin library for the RO-problems aunder TALight. In this way it better supports the in site integration of the code of the checking services of those TALight problems built to support exercises to be deployed within jupyther notebooks.\n",
    "\n",
    "from sys import stderr\n",
    "from typing import Optional, List, Dict, Callable\n",
    "import termcolor \n",
    "from ansi2html import Ansi2HTMLConverter\n",
    "ansi2html = Ansi2HTMLConverter(inline = True)\n",
    "\n",
    "class std_eval_feedback:\n",
    "    def __init__(self, ENV):\n",
    "        self.color_implementation = ENV[\"color_implementation\"]\n",
    "        self.new_line = '\\n' if ENV[\"color_implementation\"] == \"ANSI\" else '\\\\n'\n",
    "        self.with_positive_enforcement = ENV[\"with_positive_enforcement\"]\n",
    "        self.with_notes = ENV[\"with_notes\"]\n",
    "        self.task_number = ENV[\"task\"]\n",
    "        self.pt_tot = ENV[\"pt_tot\"]\n",
    "        self.pt_formato_OK = ENV[\"pt_formato_OK\"]\n",
    "        self.pt_feasibility_OK = ENV[\"pt_feasibility_OK\"]\n",
    "        self.pt_consistency_OK = ENV[\"pt_consistency_OK\"]\n",
    "        self.feedback_so_far = \"\"\n",
    "        self.completed_feedback = False\n",
    "\n",
    "    def feedback_append(self, last_feedback_msg:str):\n",
    "        self.feedback_so_far += last_feedback_msg\n",
    "        #print(f\"current feedback={self.feedback_so_far}\",file=stderr)\n",
    "    \n",
    "    def colored(self, msg_text, *msg_rendering):\n",
    "        if self.color_implementation == None:\n",
    "            return msg_text\n",
    "        if len(msg_rendering) == 0:\n",
    "            ANSI_msg = msg_text\n",
    "        else:\n",
    "            if type(msg_rendering[-1]) == list:\n",
    "                msg_style = msg_rendering[-1]\n",
    "                msg_colors = msg_rendering[:-1]\n",
    "            else:\n",
    "                msg_style = []\n",
    "                msg_colors = msg_rendering\n",
    "            ANSI_msg = termcolor.colored(msg_text, *msg_colors, attrs=msg_style)\n",
    "        if self.color_implementation == 'ANSI':\n",
    "            colored_msg = ANSI_msg\n",
    "        elif self.color_implementation == 'html':\n",
    "            colored_msg = ansi2html.convert(ANSI_msg.replace(\">\", \"&gt;\").replace(\"<\", \"&lt;\"), full=False).replace(self.new_line, \"\\n<br/>\")\n",
    "        else:\n",
    "            assert self.color_implementation == None\n",
    "        return colored_msg\n",
    "\n",
    "    def evaluation_format(self, explanation, pt_safe:Optional[int] = None,pt_out:Optional[int] = None):\n",
    "        pt_maybe = self.pt_tot-(pt_safe if pt_safe != None else 0)-(pt_out if pt_out != None else 0)\n",
    "        index_pt=self.task_number-1\n",
    "        arr_point[index_pt]=pt_safe\n",
    "        file = open(\"points.txt\", \"w\")\n",
    "        file.write(str(arr_point))\n",
    "        file.close()\n",
    "        self.summary_of_scores = self.colored(f\"{self.new_line}Totalizzi \", [\"bold\"]) + \\\n",
    "                               self.colored(f\"[punti sicuri: {pt_safe}]\", \"green\", [\"bold\"]) + \", \" + \\\n",
    "                               self.colored(f\"[punti aggiuntivi possibili: {pt_maybe}]\", \"blue\", [\"bold\"]) + \", \" + \\\n",
    "                               self.colored(f\"[punti fuori portata: {pt_out}]\", \"red\", [\"bold\"])\n",
    "        self.last_explanation = self.colored(f\"{self.new_line}Spiegazione: \", \"cyan\", [\"bold\"]) + explanation + self.colored(f\"{self.new_line}\")\n",
    "        self.feedback_append(self.summary_of_scores)\n",
    "        self.feedback_append(self.last_explanation)\n",
    "        if pt_safe is None:\n",
    "            pt_safe = 0\n",
    "        if pt_out is None:\n",
    "            pt_out = 0\n",
    "        self.completed_feedback = {'pt_safe':pt_safe,'pt_maybe':pt_maybe,'pt_out':pt_out,'pt_available':self.pt_tot,'feedback_string':self.feedback_so_far,'summary-of-scores':self.summary_of_scores,'last_explanation':self.last_explanation}\n",
    "\n",
    "   \n",
    "    class goal:\n",
    "        def __init__(self, goal_std_name, ad_hoc_name, answer_object):\n",
    "            self.std_name = goal_std_name\n",
    "            self.alias = ad_hoc_name\n",
    "            self.answ = answer_object\n",
    "        def __repr__(self):\n",
    "            return f'Object goal(self.std_name={self.std_name}, self.alias={self.alias}, self.answ={self.answ})'\n",
    "        \n",
    "    def load(self, long_answer_dict:Dict):\n",
    "        goals = {}\n",
    "        for std_name,obj_with_alias in long_answer_dict.items():\n",
    "            goals[std_name] = self.goal(std_name, ad_hoc_name=obj_with_alias[1], answer_object=obj_with_alias[0])\n",
    "        return goals\n",
    "\n",
    "    \n",
    "    def voice_NO(self, voice:str, explanation:str):\n",
    "        self.feedback_append(f\"• {voice}: \"+self.colored(f\"NO.\", \"red\", [\"bold\"])+self.colored(f\".\", \"red\")+self.colored(f\".\", \"magenta\") + self.colored(f\".(motivo: \", \"magenta\", [\"bold\"]) + explanation + self.colored(\")\", \"magenta\", [\"bold\"]) + self.colored(self.new_line))\n",
    "\n",
    "    def voice_OK(self, voice:str, positive_enforcement:str, note:str):\n",
    "        self.feedback_append(f\"• {voice}: \"+self.colored(f\"OK\", \"green\", [\"bold\"]))\n",
    "        if self.with_positive_enforcement:\n",
    "            self.feedback_append(self.colored(\".\", \"green\", [\"bold\"])+self.colored(f\"..(infatti: \", \"green\") + positive_enforcement + self.colored(\")\", \"green\"))\n",
    "        if self.with_notes:\n",
    "            self.feedback_append(self.colored(\".\", \"green\", [\"bold\"])+self.colored(f\".\", \"green\")+self.colored(f\".\", \"cyan\") + self.colored(f\".(nota: \", \"cyan\", [\"bold\"]) + note + self.colored(\")\", \"cyan\", [\"bold\"]))\n",
    "        self.feedback_append(self.colored(self.new_line))\n",
    "        \n",
    "    def format_NO(self, goal, explanation):\n",
    "        self.voice_NO(f\"Formato di `{goal.alias}`\", explanation)\n",
    "        self.evaluation_format(explanation, pt_safe=None,pt_out=self.pt_tot)\n",
    "        return False\n",
    "        \n",
    "    def format_OK(self, goal, positive_enforcement, note):\n",
    "        self.voice_OK(f\"Formato di `{goal.alias}`\", positive_enforcement, note)\n",
    "        \n",
    "    def feasibility_NO(self, goal, explanation):\n",
    "        self.voice_NO(f\"Ammissibilità di `{goal.alias}`\", explanation)\n",
    "        self.evaluation_format(explanation, pt_safe=self.pt_formato_OK,pt_out=self.pt_tot-self.pt_formato_OK)\n",
    "        return False\n",
    "        \n",
    "    def feasibility_OK(self, goal, positive_enforcement, note):\n",
    "        self.voice_OK(f\"Ammissibilità di `{goal.alias}`\", positive_enforcement, note)\n",
    "        \n",
    "    def consistency_NO(self, goals, explanation):\n",
    "        self.voice_NO(f\"Consistenza tra `{'` e `'.join([goal_std_name for goal_std_name in goals])}`\", explanation)\n",
    "        self.evaluation_format(explanation, pt_safe=self.pt_formato_OK+self.pt_feasibility_OK,pt_out=self.pt_tot-self.pt_formato_OK-self.pt_feasibility_OK)\n",
    "        return False\n",
    "        \n",
    "    def consistency_OK(self, goals, positive_enforcement, note):\n",
    "        self.voice_OK(f\"Consistenza tra `{'` e `'.join([goal_std_name for goal_std_name in goals])}`\", positive_enforcement, note)\n",
    "\n",
    "            \n",
    "    def feedback_when_all_checks_passed(self):\n",
    "        self.evaluation_format(f\"Quanto sottomesso{'' if self.task_number < 0 else ' per la Richiesta '+str(self.task_number)} ha superato tutti i miei controlli. Ovviamente in sede di esame non posso esprimermi sull'ottimalità di valori e di soluzioni immesse. Il mio controllo e supporto si è limitato alla compatibilità di formato, all'ammissibilità, e alla consistenza dei dati immessi.\", pt_safe=self.pt_formato_OK + self.pt_feasibility_OK + self.pt_consistency_OK,pt_out=0)\n",
    "        return self.completed_feedback\n",
    "\n",
    "# defining the base class verify_submission_gen (valid for any RO_* problem)\n",
    "\n",
    "from sys import stderr\n",
    "from typing import Optional, List, Dict, Callable\n",
    "from types import SimpleNamespace\n",
    "\n",
    "class verify_submission_gen:\n",
    "    def __init__(self, SEF,input_data_assigned:Dict, long_answer_dict:Dict):\n",
    "        self.I = SimpleNamespace(**input_data_assigned)\n",
    "        self.goals = SEF.load(long_answer_dict)\n",
    "        self.long_answer_dict = long_answer_dict\n",
    "\n",
    "    def verify_format(self, SEF):\n",
    "        \"\"\"In realtà questo tipo di controllo è attualmente demandato alla funzione check_and_standardization_of_request_answer_consistency  del modluo RO_std_io_lib.py\n",
    "           Preferisco tuttavia richiamarlo quì perchè:\n",
    "              1. non è del tutto chiaro (anche in relazione all'esperienza d'uso che avviene dentro i fogli Jupyther, il messaggio prodotto dalla funzione check_and_standardization_of_request_answer_consistenc  al momento è troppo severo, e d'altronde sono messaggi del modulo multilanguage che in questo momento non intedo toccare, ma posso duplicare il codice di quella parte e forse quella è la soluzione migliore, ma a quel punto la domanda è perchè non portare quì il controllo di tipo  togliendolo da  check_and_standardization_of_request_answer_consisten) quale possa essere il suo miglior collocamento\n",
    "              2. come promemoria che vogliamo inventarci modi per portare a fattor comune controlli da problemi diversi.\n",
    "           Tuttavia, per quanto riguarda 2, la via importante è che in questo modulo si definiscano delle funzioni general purpose per fare controlli sufficientemente generali (per definizione di un qualche linguaggio minimale e/oppure passando ad esse funzioni che fanno il controllo). Ma certo trovare delle buone soluzioni (con intelligenti compromessi) avrebbe il doppio vantaggio di sgravare il lavoro del problem maker e di offrire un'esperienza d'uso più standardizzata allo studente/problem solver.\n",
    "        \"\"\"\n",
    "        for g in self.goals:\n",
    "            if self.long_answer_dict[g] == 'int':\n",
    "                if type(g.answ) != int:\n",
    "                    return SEF.format_NO(g, f\"Come `{g.alias}` hai immesso `{g.answ}` dove era invece richiesto di immettere un intero.\")\n",
    "                SEF.format_OK(g, f\"come `{g.alias}` hai immesso un intero come richiesto\", f\"ovviamente durante lo svolgimento dell'esame non posso dirti se l'intero immesso sia poi la risposta corretta, ma il formato è corretto\")            \n",
    "        return True\n",
    "                \n",
    "    def verify_feasibility(self, SEF):\n",
    "        return True\n",
    "                \n",
    "    def verify_consistency(self, SEF):\n",
    "        return True\n",
    "                \n",
    "    def set_up_and_cash_handy_data(self):\n",
    "        pass\n",
    "\n",
    "    def verify_submission(self, SEF):\n",
    "        if not self.verify_format(SEF):\n",
    "            return SEF.completed_feedback\n",
    "        self.set_up_and_cash_handy_data()\n",
    "        if not self.verify_feasibility(SEF):\n",
    "            return SEF.completed_feedback\n",
    "        if not self.verify_consistency(SEF):\n",
    "            return SEF.completed_feedback\n",
    "        return SEF.feedback_when_all_checks_passed()\n",
    "# extending class verify_submission_gen with ad-hoc virtual methods for problem knapsack\n",
    "\n",
    "from typing import Optional, List, Dict, Tuple\n",
    "from types import SimpleNamespace\n",
    "\n",
    "class verify_submission_problem_specific(verify_submission_gen):\n",
    "    def __init__(self, SEF,input_data_assigned:Dict, long_answer_dict:Dict):\n",
    "        super().__init__(SEF,input_data_assigned, long_answer_dict)\n",
    "\n",
    "    def verify_format(self, SEF):\n",
    "        if not super().verify_format(SEF):\n",
    "            return False\n",
    "        if 'opt_val' in self.goals:\n",
    "            g = self.goals['opt_val']\n",
    "            if type(g.answ) != int:\n",
    "                return SEF.format_NO(g, f\"Come `{g.alias}` hai immesso `{g.answ}` dove era invece richiesto di immettere un intero.\")\n",
    "            SEF.format_OK(g, f\"come `{g.alias}` hai immesso un intero come richiesto\", f\"ovviamente durante lo svolgimento dell'esame non posso dirti se l'intero immesso sia poi la risposta corretta, ma il formato è corretto\")            \n",
    "        if 'num_opt_sols' in self.goals:\n",
    "            g = self.goals['num_opt_sols']\n",
    "            if type(g.answ) != int:\n",
    "                return SEF.format_NO(g, f\"Come `{g.alias}` hai immesso `{g.answ}` dove era invece richiesto di immettere un intero.\")\n",
    "            SEF.format_OK(g, f\"come `{g.alias}` hai immesso un intero come richiesto\", f\"ovviamente durante lo svolgimento dell'esame non posso dirti se l'intero immesso sia poi la risposta corretta, ma il formato è corretto\")            \n",
    "        if 'opt_sol' in self.goals:\n",
    "            g = self.goals['opt_sol']\n",
    "            if type(g.answ) != list:\n",
    "                return SEF.format_NO(g, f\"Come `{g.alias}` è richiesto si inserisca una lista di oggetti (esempio ['{self.I.labels[0]}','{self.I.labels[2]}']). Hai invece immesso `{g.answ}`.\")\n",
    "            for ele in g.answ:\n",
    "                if ele not in self.I.labels:\n",
    "                    return SEF.format_NO(g, f\"Ogni oggetto che collochi nella lista `{g.alias}` deve essere uno degli elementi disponibili. L'elemento `{ele}` da tè inserito non è tra questi. Gli oggetti disponibili sono {self.I.labels}.\")\n",
    "            SEF.format_OK(g, f\"come `{g.alias}` hai immesso un sottoinsieme degli oggetti dell'istanza originale\", f\"resta da stabilire l'ammissibilità di `{g.alias}`\")\n",
    "        return True\n",
    "                \n",
    "    def set_up_and_cash_handy_data(self):\n",
    "        if 'opt_sol' in self.goals:\n",
    "            self.sum_vals = sum([val for ele,cost,val in zip(self.I.labels,self.I.costs,self.I.vals) if ele in self.goals['opt_sol'].answ])\n",
    "            self.sum_costs = sum([cost for ele,cost,val in zip(self.I.labels,self.I.costs,self.I.vals) if ele in self.goals['opt_sol'].answ])\n",
    "            \n",
    "    def verify_feasibility(self, SEF):\n",
    "        if not super().verify_feasibility(SEF):\n",
    "            return False\n",
    "        if 'opt_sol' in self.goals:\n",
    "            g = self.goals['opt_sol']\n",
    "            for ele in g.answ:\n",
    "                if ele in self.I.forced_out:\n",
    "                    return SEF.feasibility_NO(g, f\"L'oggetto `{ele}` da tè inserito nella lista `{g.alias}` è tra quelli proibiti. Gli oggetti proibiti per la Richiesta {str(SEF.task_number)}, sono {self.I.forced_out}.\")\n",
    "            for ele in self.I.forced_in:\n",
    "                if ele not in g.answ:\n",
    "                    return SEF.feasibility_NO(g, f\"Nella lista `{g.alias}` hai dimenticato di inserire l'oggetto `{ele}` che invece è forzato. Gli oggetti forzati per la Richiesta {str(SEF.task_number)} sono {self.I.forced_in}.\")\n",
    "            if self.sum_costs > self.I.Knapsack_Capacity:\n",
    "                return SEF.feasibility_NO(g, f\"La tua soluzione in `{g.alias}` ha costo {self.sum_costs} > Knapsack_Capacity e quindi NON è ammissibile in quanto fora il budget per la Richiesta {str(SEF.task_number)}. La soluzione da tè inserita ricomprende il sottoinsieme di oggetti `{g.alias}`= {g.answ}.\")\n",
    "            SEF.feasibility_OK(g, f\"come `{g.alias}` hai immesso un sottoinsieme degli oggetti dell'istanza originale\", f\"resta da stabilire l'ottimalità di `{g.alias}`\")\n",
    "        return True\n",
    "                \n",
    "    def verify_consistency(self, SEF):\n",
    "        if not super().verify_consistency(SEF):\n",
    "            return False\n",
    "        if 'opt_val' in self.goals and 'opt_sol' in self.goals:\n",
    "            g_val = self.goals['opt_val']; g_sol = self.goals['opt_sol'];\n",
    "            if self.sum_vals != g_val.answ:\n",
    "                return SEF.consistency_NO(['opt_val','opt_sol'], f\"Il valore totale della soluzione immessa in `{g_sol.alias}` è {self.sum_vals}, non {g_val.answ} come hai invece immesso in `{g_val.alias}`. La soluzione (ammissibile) che hai immesso è `{g_sol.alias}`={g_sol.answ}.\")\n",
    "            SEF.consistency_OK(['opt_val','opt_sol'], f\"{g_val.alias}={g_val.answ} = somma dei valori sugli oggetti in `{g_sol.alias}`.\", \"\")\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ce4297",
   "metadata": {
    "deletable": false,
    "editable": true,
    "hide_input": false,
    "init_cell": true,
    "tags": []
   },
   "source": [
    "Si consideri uno zaino di capienza $CapacityMax$ = 36 e i seguenti oggetti ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'L', 'M', 'N'], aventi i seguenti costi [15, 16, 17, 11, 13, 5, 7, 3, 1, 12, 9, 7] e valori [50, 52, 54, 40, 45, 17, 18, 7, 8, 42, 30, 22]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6af253e",
   "metadata": {
    "deletable": false,
    "editable": true,
    "hide_input": false,
    "tags": [
     "runcell",
     "noexport"
    ]
   },
   "source": [
    "__Richiesta 1 [40 punti]__: Tra i sottoinsiemi di oggetti di peso complessivo non eccedente $CapacityMax$= 36 fornirne uno in cui sia massima la somma dei valori."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79d29c0",
   "metadata": {
    "deletable": false,
    "editable": true,
    "hide_input": false,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "#Inserisci la tua risposta in forma di lista di oggetti da prendere (esempio: ['C','F','A'])\n",
    "opt_sol1=[]\n",
    "#Specificare il valore della soluzione introdotta (un intero, la somma dei valori degli oggetti presi):\n",
    "opt_val1=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ea92bb",
   "metadata": {
    "deletable": false,
    "editable": true,
    "hide_input": false,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "needed_varnames = ['opt_sol1', 'opt_val1', 'DPtable1', 'forced_out1', 'forced_in1']\n",
    "for varname in needed_varnames:\n",
    "    if varname not in locals():\n",
    "        exec(f'{varname} = None')\n",
    "# launch all requests for evaluation of the answer objects and get the feedback\n",
    "\n",
    "def check_checkers_agreement(URL_1_NAME, TAL_feedback_dict1, feedback_dict2, URL_2_NAME=None):\n",
    "    if URL_2_NAME is None:\n",
    "        term_of_comparison = \"that of the local King Arthur placed in the notebook\"\n",
    "    else:\n",
    "        term_of_comparison = \"that from server {URL_2_NAME}\"\n",
    "    if TAL_feedback_dict1 != None and feedback_dict2 != None and 'pt_safe' in TAL_feedback_dict1 and 'pt_maybe' in TAL_feedback_dict1 and 'pt_out' in TAL_feedback_dict1 and 'pt_safe' in feedback_dict2:\n",
    "        if TAL_feedback_dict1['pt_safe'] != feedback_dict2['pt_safe']:\n",
    "            print(f\"\\n\\nWARNING: the TALight opinion (pt_safe={TAL_feedback_dict1['pt_safe']}) from server `{URL_1_NAME}` differs from {term_of_comparison} (pt_safe={feedback_dict2['pt_safe']})\")\n",
    "        if TAL_feedback_dict1['pt_maybe'] != feedback_dict2['pt_maybe']:\n",
    "            print(f\"\\n\\nWARNING: the TALight opinion (pt_maybe={TAL_feedback_dict1['pt_maybe']}) from server `{URL_1_NAME}` differs from {term_of_comparison} (pt_maybe={feedback_dict2['pt_maybe']})\")\n",
    "        if TAL_feedback_dict1['pt_out'] != feedback_dict2['pt_out']:\n",
    "            print(f\"\\n\\nWARNING: the TALight opinion (pt_out={TAL_feedback_dict1['pt_out']}) from server `{URL_1_NAME}` differs from {term_of_comparison} (pt_out={feedback_dict2['pt_out']})\")\n",
    "\n",
    "\n",
    "def verify_submission(TALight_problem_name:str,checkers:List, task_dict:Dict, input_data_assigned:Dict, long_answer_dict:Dict):\n",
    "    #print(\"\\n\\n\\n\" f\"task_dict={task_dict}\" \"\\n\\n\\n\", file=stderr)\n",
    "    task_dict['with_positive_enforcement'] = '1' if 'positive_enforcements' in settings['FEEDBACKS_TO_STUDENT'] else '0'\n",
    "    task_dict['with_notes'] = '1' if 'cautious_notes' in settings['FEEDBACKS_TO_STUDENT'] else '0'\n",
    "    task_dict['as_yaml_with_points'] = '1'\n",
    "    task_dict['color_implementation'] = 'html'\n",
    "    task_dict['with_output_files'] = '0'\n",
    "    if settings['CHECKERS_INVOLVED'] == 'ONLY_THE_ONE_EMBEDDED_IN_NOTEBOOK':\n",
    "        checkers=['embedded_in_notebook']\n",
    "    elif settings['CHECKERS_INVOLVED'] == 'ONLY_THE_TALIGHT_ONES':\n",
    "        checkers=['TALight']\n",
    "    elif settings['CHECKERS_INVOLVED'] == 'BOTH':\n",
    "        checkers=['TALight', 'embedded_in_notebook']\n",
    "    elif settings['CHECKERS_INVOLVED'] == 'ONLY_FIRST_IN_LIST_FROM_YAML_LONGFILE':\n",
    "        checkers=checkers[0]\n",
    "    elif settings['CHECKERS_INVOLVED'] != 'THOSE_IN_LIST_FROM_YAML_LONGFILE':\n",
    "        print(f\"Unknown option `{settings['CHECKERS_INVOLVED']}` for the setting `CHECKERS_INVOLVED`\")\n",
    "        return\n",
    "    if 'TALight' in checkers:\n",
    "        if len(settings['RTAL_URLS']) == 0:\n",
    "            print(\"Invalid Settings File: `CHECKERS_INVOLVED`=`{settings['CHECKERS_INVOLVED']}` is in contraddiction with `RTAL_URLS` set to the empty list\")\n",
    "            return\n",
    "        answer_dict = { val[1]:val[0] for val in long_answer_dict.values() }\n",
    "        alias_dict = { val[1]:key for key, val in long_answer_dict.items() }\n",
    "        rtalargs_dict = task_dict\n",
    "        rtalargs_dict['input_data_assigned'] = input_data_assigned\n",
    "        rtalargs_dict['alias_dict'] = alias_dict\n",
    "        rtalargs_dict['answer_dict'] = answer_dict\n",
    "        URL_NAME = settings['RTAL_URLS'][0]\n",
    "        URL = settings[URL_NAME]\n",
    "        TAL_feedback_dict = rtal_connect(URL, TALight_problem_name, 'check', rtalargs_dict=rtalargs_dict, output_files_local_folder='output_files_TALight', rtaltoken=RTAL_PRIVATE_ACCESS_TOKEN)\n",
    "        monitor_what_submitted_to_rtald(TALight_problem_name,rtalargs_dict)\n",
    "        print(f\"Here is what the TALight checker thinks (and has stored on the server `{URL_NAME}`) about your submission:\")\n",
    "        if TAL_feedback_dict != None:\n",
    "            display(TAL_feedback_dict['feedback_string'])\n",
    "        elif 'signal_when_no_feedback_has_been_produced' in setting['RTAL_INTERFACE_VERBOSITY']:\n",
    "            print(\"The Feedback received from the server is EMPTY\")\n",
    "        for next_URL_NAME in settings['RTAL_URLS'][1:]:\n",
    "            URL = settings[next_URL_NAME]\n",
    "            print(f\"Next we pose the very same request also to the server `{next_URL_NAME}`:\")\n",
    "            next_TAL_feedback_dict = rtal_connect(URL, TALight_problem_name, 'check', rtalargs_dict=rtalargs_dict, output_files_local_folder='output_files_TALight', rtaltoken=RTAL_PRIVATE_ACCESS_TOKEN)\n",
    "            print(f\"Here is what the TALight checker thinks (and has stored on the server `{next_URL_NAME}`) about your submission:\")\n",
    "            if next_TAL_feedback_dict != None:\n",
    "                display(next_TAL_feedback_dict['feedback_string'])\n",
    "                check_checkers_agreement(URL_NAME,TAL_feedback_dict,next_TAL_feedback_dict,next_URL_NAME)\n",
    "            if next_TAL_feedback_dict != None:\n",
    "                display(TAL_feedback_dict['feedback_string'])\n",
    "            elif 'signal_when_no_feedback_has_been_produced' in setting['RTAL_INTERFACE_VERBOSITY']:\n",
    "                print(\"The Feedback received from the server is EMPTY\")\n",
    "\n",
    "    if 'embedded_in_notebook' in checkers:\n",
    "        print(\"Here is an independent evaluation (just in case the TALight server is down or unreachable):\")\n",
    "        SEF = std_eval_feedback(task_dict)\n",
    "        KingArthur = verify_submission_problem_specific(SEF, input_data_assigned, long_answer_dict)\n",
    "        embedded_feedback_dict = KingArthur.verify_submission(SEF)\n",
    "        #print(f\"feedback_dict={feedback_dict}\", file=stderr)\n",
    "        display(HTML(embedded_feedback_dict['feedback_string']))\n",
    "        if 'TALight' in checkers and settings['CHECK_AGREEMENT_OF_CHECKERS']:\n",
    "            check_checkers_agreement(URL_NAME,TAL_feedback_dict,embedded_feedback_dict)\n",
    "verify_submission(TALight_problem_name='RO_knapsack',checkers=['TALight', 'embedded_in_notebook'],task_dict={'task': 1, 'pt_tot': 40, 'pt_formato_OK': 0, 'pt_feasibility_OK': 1, 'pt_consistency_OK': 0}, input_data_assigned={'labels':['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'L', 'M', 'N'],'costs':[15, 16, 17, 11, 13, 5, 7, 3, 1, 12, 9, 7],'vals':[50, 52, 54, 40, 45, 17, 18, 7, 8, 42, 30, 22],'Knapsack_Capacity':36,'forced_out':[],'forced_in':[],'partialDPtable':[],}, long_answer_dict={'opt_sol':(opt_sol1,'opt_sol1'),'opt_val':(opt_val1,'opt_val1'),})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c1e379",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hide_input": true,
    "init_cell": true,
    "tags": [
     "noexport"
    ]
   },
   "outputs": [],
   "source": [
    "loader_main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1953d4d8",
   "metadata": {
    "deletable": false,
    "editable": true,
    "hide_input": false,
    "tags": [
     "runcell",
     "noexport"
    ]
   },
   "source": [
    "__Richiesta 2 [40 punti]__: Tra i sottoinsiemi di oggetti di peso complessivo non eccedente <b>la capacità 32</b>, fornirne uno in cui sia massima la somma dei valori."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f162c40",
   "metadata": {
    "deletable": false,
    "editable": true,
    "hide_input": false,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "#Inserisci in `opt_sol2` la lista degli oggetti da prendere (esempio: ['N', 'M', 'L', 'I', 'H', 'G', 'E', 'A'])\n",
    "opt_sol2=[]\n",
    "#Immetti in `opt_val2` il valore della soluzione introdotta (un intero, la somma dei valori degli oggetti presi)\n",
    "opt_val2=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f6cefb",
   "metadata": {
    "deletable": false,
    "editable": true,
    "hide_input": false,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "needed_varnames = ['opt_sol2', 'opt_val2', 'DPtable2', 'forced_out2', 'forced_in2']\n",
    "for varname in needed_varnames:\n",
    "    if varname not in locals():\n",
    "        exec(f'{varname} = None')\n",
    "# launch all requests for evaluation of the answer objects and get the feedback\n",
    "\n",
    "def check_checkers_agreement(URL_1_NAME, TAL_feedback_dict1, feedback_dict2, URL_2_NAME=None):\n",
    "    if URL_2_NAME is None:\n",
    "        term_of_comparison = \"that of the local King Arthur placed in the notebook\"\n",
    "    else:\n",
    "        term_of_comparison = \"that from server {URL_2_NAME}\"\n",
    "    if TAL_feedback_dict1 != None and feedback_dict2 != None and 'pt_safe' in TAL_feedback_dict1 and 'pt_maybe' in TAL_feedback_dict1 and 'pt_out' in TAL_feedback_dict1 and 'pt_safe' in feedback_dict2:\n",
    "        if TAL_feedback_dict1['pt_safe'] != feedback_dict2['pt_safe']:\n",
    "            print(f\"\\n\\nWARNING: the TALight opinion (pt_safe={TAL_feedback_dict1['pt_safe']}) from server `{URL_1_NAME}` differs from {term_of_comparison} (pt_safe={feedback_dict2['pt_safe']})\")\n",
    "        if TAL_feedback_dict1['pt_maybe'] != feedback_dict2['pt_maybe']:\n",
    "            print(f\"\\n\\nWARNING: the TALight opinion (pt_maybe={TAL_feedback_dict1['pt_maybe']}) from server `{URL_1_NAME}` differs from {term_of_comparison} (pt_maybe={feedback_dict2['pt_maybe']})\")\n",
    "        if TAL_feedback_dict1['pt_out'] != feedback_dict2['pt_out']:\n",
    "            print(f\"\\n\\nWARNING: the TALight opinion (pt_out={TAL_feedback_dict1['pt_out']}) from server `{URL_1_NAME}` differs from {term_of_comparison} (pt_out={feedback_dict2['pt_out']})\")\n",
    "\n",
    "\n",
    "def verify_submission(TALight_problem_name:str,checkers:List, task_dict:Dict, input_data_assigned:Dict, long_answer_dict:Dict):\n",
    "    #print(\"\\n\\n\\n\" f\"task_dict={task_dict}\" \"\\n\\n\\n\", file=stderr)\n",
    "    task_dict['with_positive_enforcement'] = '1' if 'positive_enforcements' in settings['FEEDBACKS_TO_STUDENT'] else '0'\n",
    "    task_dict['with_notes'] = '1' if 'cautious_notes' in settings['FEEDBACKS_TO_STUDENT'] else '0'\n",
    "    task_dict['as_yaml_with_points'] = '1'\n",
    "    task_dict['color_implementation'] = 'html'\n",
    "    task_dict['with_output_files'] = '0'\n",
    "    if settings['CHECKERS_INVOLVED'] == 'ONLY_THE_ONE_EMBEDDED_IN_NOTEBOOK':\n",
    "        checkers=['embedded_in_notebook']\n",
    "    elif settings['CHECKERS_INVOLVED'] == 'ONLY_THE_TALIGHT_ONES':\n",
    "        checkers=['TALight']\n",
    "    elif settings['CHECKERS_INVOLVED'] == 'BOTH':\n",
    "        checkers=['TALight', 'embedded_in_notebook']\n",
    "    elif settings['CHECKERS_INVOLVED'] == 'ONLY_FIRST_IN_LIST_FROM_YAML_LONGFILE':\n",
    "        checkers=checkers[0]\n",
    "    elif settings['CHECKERS_INVOLVED'] != 'THOSE_IN_LIST_FROM_YAML_LONGFILE':\n",
    "        print(f\"Unknown option `{settings['CHECKERS_INVOLVED']}` for the setting `CHECKERS_INVOLVED`\")\n",
    "        return\n",
    "    if 'TALight' in checkers:\n",
    "        if len(settings['RTAL_URLS']) == 0:\n",
    "            print(\"Invalid Settings File: `CHECKERS_INVOLVED`=`{settings['CHECKERS_INVOLVED']}` is in contraddiction with `RTAL_URLS` set to the empty list\")\n",
    "            return\n",
    "        answer_dict = { val[1]:val[0] for val in long_answer_dict.values() }\n",
    "        alias_dict = { val[1]:key for key, val in long_answer_dict.items() }\n",
    "        rtalargs_dict = task_dict\n",
    "        rtalargs_dict['input_data_assigned'] = input_data_assigned\n",
    "        rtalargs_dict['alias_dict'] = alias_dict\n",
    "        rtalargs_dict['answer_dict'] = answer_dict\n",
    "        URL_NAME = settings['RTAL_URLS'][0]\n",
    "        URL = settings[URL_NAME]\n",
    "        TAL_feedback_dict = rtal_connect(URL, TALight_problem_name, 'check', rtalargs_dict=rtalargs_dict, output_files_local_folder='output_files_TALight', rtaltoken=RTAL_PRIVATE_ACCESS_TOKEN)\n",
    "        monitor_what_submitted_to_rtald(TALight_problem_name,rtalargs_dict)\n",
    "        print(f\"Here is what the TALight checker thinks (and has stored on the server `{URL_NAME}`) about your submission:\")\n",
    "        if TAL_feedback_dict != None:\n",
    "            display(TAL_feedback_dict['feedback_string'])\n",
    "        elif 'signal_when_no_feedback_has_been_produced' in setting['RTAL_INTERFACE_VERBOSITY']:\n",
    "            print(\"The Feedback received from the server is EMPTY\")\n",
    "        for next_URL_NAME in settings['RTAL_URLS'][1:]:\n",
    "            URL = settings[next_URL_NAME]\n",
    "            print(f\"Next we pose the very same request also to the server `{next_URL_NAME}`:\")\n",
    "            next_TAL_feedback_dict = rtal_connect(URL, TALight_problem_name, 'check', rtalargs_dict=rtalargs_dict, output_files_local_folder='output_files_TALight', rtaltoken=RTAL_PRIVATE_ACCESS_TOKEN)\n",
    "            print(f\"Here is what the TALight checker thinks (and has stored on the server `{next_URL_NAME}`) about your submission:\")\n",
    "            if next_TAL_feedback_dict != None:\n",
    "                display(next_TAL_feedback_dict['feedback_string'])\n",
    "                check_checkers_agreement(URL_NAME,TAL_feedback_dict,next_TAL_feedback_dict,next_URL_NAME)\n",
    "            if next_TAL_feedback_dict != None:\n",
    "                display(TAL_feedback_dict['feedback_string'])\n",
    "            elif 'signal_when_no_feedback_has_been_produced' in setting['RTAL_INTERFACE_VERBOSITY']:\n",
    "                print(\"The Feedback received from the server is EMPTY\")\n",
    "\n",
    "    if 'embedded_in_notebook' in checkers:\n",
    "        print(\"Here is an independent evaluation (just in case the TALight server is down or unreachable):\")\n",
    "        SEF = std_eval_feedback(task_dict)\n",
    "        KingArthur = verify_submission_problem_specific(SEF, input_data_assigned, long_answer_dict)\n",
    "        embedded_feedback_dict = KingArthur.verify_submission(SEF)\n",
    "        #print(f\"feedback_dict={feedback_dict}\", file=stderr)\n",
    "        display(HTML(embedded_feedback_dict['feedback_string']))\n",
    "        if 'TALight' in checkers and settings['CHECK_AGREEMENT_OF_CHECKERS']:\n",
    "            check_checkers_agreement(URL_NAME,TAL_feedback_dict,embedded_feedback_dict)\n",
    "verify_submission(TALight_problem_name='RO_knapsack',checkers=['TALight', 'embedded_in_notebook'],task_dict={'task': 2, 'pt_tot': 40, 'pt_formato_OK': 0, 'pt_feasibility_OK': 1, 'pt_consistency_OK': 0}, input_data_assigned={'labels':['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'L', 'M', 'N'],'costs':[15, 16, 17, 11, 13, 5, 7, 3, 1, 12, 9, 7],'vals':[50, 52, 54, 40, 45, 17, 18, 7, 8, 42, 30, 22],'Knapsack_Capacity':32,'forced_out':[],'forced_in':[],'partialDPtable':[],}, long_answer_dict={'opt_sol':(opt_sol2,'opt_sol2'),'opt_val':(opt_val2,'opt_val2'),})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04f5180",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hide_input": true,
    "init_cell": true,
    "tags": [
     "noexport"
    ]
   },
   "outputs": [],
   "source": [
    "loader_main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db8b676",
   "metadata": {
    "deletable": false,
    "editable": true,
    "hide_input": false,
    "tags": [
     "runcell",
     "noexport"
    ]
   },
   "source": [
    "__Richiesta 3 [40 punti]__: Tra i sottoinsiemi di oggetti di peso complessivo non eccedente <b>la capacità 30</b>, fornirne uno in cui sia massima la somma dei valori."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51038e9c",
   "metadata": {
    "deletable": false,
    "editable": true,
    "hide_input": false,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "#Inserisci in `opt_sol3` la lista degli oggetti da prendere (esempio: ['N', 'M', 'L', 'I', 'H', 'D', 'G'])\n",
    "opt_sol3=[]\n",
    "#Immetti in `opt_val3` il valore della soluzione introdotta (un intero, la somma dei valori degli oggetti presi)\n",
    "opt_val3=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206c1061",
   "metadata": {
    "deletable": false,
    "editable": true,
    "hide_input": false,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "needed_varnames = ['opt_sol3', 'opt_val3', 'DPtable3', 'forced_out3', 'forced_in3']\n",
    "for varname in needed_varnames:\n",
    "    if varname not in locals():\n",
    "        exec(f'{varname} = None')\n",
    "# launch all requests for evaluation of the answer objects and get the feedback\n",
    "\n",
    "def check_checkers_agreement(URL_1_NAME, TAL_feedback_dict1, feedback_dict2, URL_2_NAME=None):\n",
    "    if URL_2_NAME is None:\n",
    "        term_of_comparison = \"that of the local King Arthur placed in the notebook\"\n",
    "    else:\n",
    "        term_of_comparison = \"that from server {URL_2_NAME}\"\n",
    "    if TAL_feedback_dict1 != None and feedback_dict2 != None and 'pt_safe' in TAL_feedback_dict1 and 'pt_maybe' in TAL_feedback_dict1 and 'pt_out' in TAL_feedback_dict1 and 'pt_safe' in feedback_dict2:\n",
    "        if TAL_feedback_dict1['pt_safe'] != feedback_dict2['pt_safe']:\n",
    "            print(f\"\\n\\nWARNING: the TALight opinion (pt_safe={TAL_feedback_dict1['pt_safe']}) from server `{URL_1_NAME}` differs from {term_of_comparison} (pt_safe={feedback_dict2['pt_safe']})\")\n",
    "        if TAL_feedback_dict1['pt_maybe'] != feedback_dict2['pt_maybe']:\n",
    "            print(f\"\\n\\nWARNING: the TALight opinion (pt_maybe={TAL_feedback_dict1['pt_maybe']}) from server `{URL_1_NAME}` differs from {term_of_comparison} (pt_maybe={feedback_dict2['pt_maybe']})\")\n",
    "        if TAL_feedback_dict1['pt_out'] != feedback_dict2['pt_out']:\n",
    "            print(f\"\\n\\nWARNING: the TALight opinion (pt_out={TAL_feedback_dict1['pt_out']}) from server `{URL_1_NAME}` differs from {term_of_comparison} (pt_out={feedback_dict2['pt_out']})\")\n",
    "\n",
    "\n",
    "def verify_submission(TALight_problem_name:str,checkers:List, task_dict:Dict, input_data_assigned:Dict, long_answer_dict:Dict):\n",
    "    #print(\"\\n\\n\\n\" f\"task_dict={task_dict}\" \"\\n\\n\\n\", file=stderr)\n",
    "    task_dict['with_positive_enforcement'] = '1' if 'positive_enforcements' in settings['FEEDBACKS_TO_STUDENT'] else '0'\n",
    "    task_dict['with_notes'] = '1' if 'cautious_notes' in settings['FEEDBACKS_TO_STUDENT'] else '0'\n",
    "    task_dict['as_yaml_with_points'] = '1'\n",
    "    task_dict['color_implementation'] = 'html'\n",
    "    task_dict['with_output_files'] = '0'\n",
    "    if settings['CHECKERS_INVOLVED'] == 'ONLY_THE_ONE_EMBEDDED_IN_NOTEBOOK':\n",
    "        checkers=['embedded_in_notebook']\n",
    "    elif settings['CHECKERS_INVOLVED'] == 'ONLY_THE_TALIGHT_ONES':\n",
    "        checkers=['TALight']\n",
    "    elif settings['CHECKERS_INVOLVED'] == 'BOTH':\n",
    "        checkers=['TALight', 'embedded_in_notebook']\n",
    "    elif settings['CHECKERS_INVOLVED'] == 'ONLY_FIRST_IN_LIST_FROM_YAML_LONGFILE':\n",
    "        checkers=checkers[0]\n",
    "    elif settings['CHECKERS_INVOLVED'] != 'THOSE_IN_LIST_FROM_YAML_LONGFILE':\n",
    "        print(f\"Unknown option `{settings['CHECKERS_INVOLVED']}` for the setting `CHECKERS_INVOLVED`\")\n",
    "        return\n",
    "    if 'TALight' in checkers:\n",
    "        if len(settings['RTAL_URLS']) == 0:\n",
    "            print(\"Invalid Settings File: `CHECKERS_INVOLVED`=`{settings['CHECKERS_INVOLVED']}` is in contraddiction with `RTAL_URLS` set to the empty list\")\n",
    "            return\n",
    "        answer_dict = { val[1]:val[0] for val in long_answer_dict.values() }\n",
    "        alias_dict = { val[1]:key for key, val in long_answer_dict.items() }\n",
    "        rtalargs_dict = task_dict\n",
    "        rtalargs_dict['input_data_assigned'] = input_data_assigned\n",
    "        rtalargs_dict['alias_dict'] = alias_dict\n",
    "        rtalargs_dict['answer_dict'] = answer_dict\n",
    "        URL_NAME = settings['RTAL_URLS'][0]\n",
    "        URL = settings[URL_NAME]\n",
    "        TAL_feedback_dict = rtal_connect(URL, TALight_problem_name, 'check', rtalargs_dict=rtalargs_dict, output_files_local_folder='output_files_TALight', rtaltoken=RTAL_PRIVATE_ACCESS_TOKEN)\n",
    "        monitor_what_submitted_to_rtald(TALight_problem_name,rtalargs_dict)\n",
    "        print(f\"Here is what the TALight checker thinks (and has stored on the server `{URL_NAME}`) about your submission:\")\n",
    "        if TAL_feedback_dict != None:\n",
    "            display(TAL_feedback_dict['feedback_string'])\n",
    "        elif 'signal_when_no_feedback_has_been_produced' in setting['RTAL_INTERFACE_VERBOSITY']:\n",
    "            print(\"The Feedback received from the server is EMPTY\")\n",
    "        for next_URL_NAME in settings['RTAL_URLS'][1:]:\n",
    "            URL = settings[next_URL_NAME]\n",
    "            print(f\"Next we pose the very same request also to the server `{next_URL_NAME}`:\")\n",
    "            next_TAL_feedback_dict = rtal_connect(URL, TALight_problem_name, 'check', rtalargs_dict=rtalargs_dict, output_files_local_folder='output_files_TALight', rtaltoken=RTAL_PRIVATE_ACCESS_TOKEN)\n",
    "            print(f\"Here is what the TALight checker thinks (and has stored on the server `{next_URL_NAME}`) about your submission:\")\n",
    "            if next_TAL_feedback_dict != None:\n",
    "                display(next_TAL_feedback_dict['feedback_string'])\n",
    "                check_checkers_agreement(URL_NAME,TAL_feedback_dict,next_TAL_feedback_dict,next_URL_NAME)\n",
    "            if next_TAL_feedback_dict != None:\n",
    "                display(TAL_feedback_dict['feedback_string'])\n",
    "            elif 'signal_when_no_feedback_has_been_produced' in setting['RTAL_INTERFACE_VERBOSITY']:\n",
    "                print(\"The Feedback received from the server is EMPTY\")\n",
    "\n",
    "    if 'embedded_in_notebook' in checkers:\n",
    "        print(\"Here is an independent evaluation (just in case the TALight server is down or unreachable):\")\n",
    "        SEF = std_eval_feedback(task_dict)\n",
    "        KingArthur = verify_submission_problem_specific(SEF, input_data_assigned, long_answer_dict)\n",
    "        embedded_feedback_dict = KingArthur.verify_submission(SEF)\n",
    "        #print(f\"feedback_dict={feedback_dict}\", file=stderr)\n",
    "        display(HTML(embedded_feedback_dict['feedback_string']))\n",
    "        if 'TALight' in checkers and settings['CHECK_AGREEMENT_OF_CHECKERS']:\n",
    "            check_checkers_agreement(URL_NAME,TAL_feedback_dict,embedded_feedback_dict)\n",
    "verify_submission(TALight_problem_name='RO_knapsack',checkers=['TALight', 'embedded_in_notebook'],task_dict={'task': 3, 'pt_tot': 40, 'pt_formato_OK': 0, 'pt_feasibility_OK': 1, 'pt_consistency_OK': 0}, input_data_assigned={'labels':['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'L', 'M', 'N'],'costs':[15, 16, 17, 11, 13, 5, 7, 3, 1, 12, 9, 7],'vals':[50, 52, 54, 40, 45, 17, 18, 7, 8, 42, 30, 22],'Knapsack_Capacity':30,'forced_out':[],'forced_in':[],'partialDPtable':[],}, long_answer_dict={'opt_sol':(opt_sol3,'opt_sol3'),'opt_val':(opt_val3,'opt_val3'),})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd7a963",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hide_input": true,
    "init_cell": true,
    "tags": [
     "noexport"
    ]
   },
   "outputs": [],
   "source": [
    "loader_main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6febc81",
   "metadata": {
    "deletable": false,
    "editable": true,
    "hide_input": false,
    "tags": [
     "runcell",
     "noexport"
    ]
   },
   "source": [
    "__Richiesta 4 [40 punti]__: Tra i sottoinsiemi di oggetti di peso complessivo non eccedente <b>la capacità 28</b>, fornirne uno in cui sia massima la somma dei valori."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792228a0",
   "metadata": {
    "deletable": false,
    "editable": true,
    "hide_input": false,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "#Inserisci in `opt_sol4` la lista degli oggetti da prendere (esempio: ['N', 'M', 'L', 'I', 'A', 'B'])\n",
    "opt_sol4=[]\n",
    "#Immetti in `opt_val4` il valore della soluzione introdotta (un intero, la somma dei valori degli oggetti presi)\n",
    "opt_val4=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9654d43e",
   "metadata": {
    "deletable": false,
    "editable": true,
    "hide_input": false,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "needed_varnames = ['opt_sol4', 'opt_val4', 'DPtable4', 'forced_out4', 'forced_in4']\n",
    "for varname in needed_varnames:\n",
    "    if varname not in locals():\n",
    "        exec(f'{varname} = None')\n",
    "# launch all requests for evaluation of the answer objects and get the feedback\n",
    "\n",
    "def check_checkers_agreement(URL_1_NAME, TAL_feedback_dict1, feedback_dict2, URL_2_NAME=None):\n",
    "    if URL_2_NAME is None:\n",
    "        term_of_comparison = \"that of the local King Arthur placed in the notebook\"\n",
    "    else:\n",
    "        term_of_comparison = \"that from server {URL_2_NAME}\"\n",
    "    if TAL_feedback_dict1 != None and feedback_dict2 != None and 'pt_safe' in TAL_feedback_dict1 and 'pt_maybe' in TAL_feedback_dict1 and 'pt_out' in TAL_feedback_dict1 and 'pt_safe' in feedback_dict2:\n",
    "        if TAL_feedback_dict1['pt_safe'] != feedback_dict2['pt_safe']:\n",
    "            print(f\"\\n\\nWARNING: the TALight opinion (pt_safe={TAL_feedback_dict1['pt_safe']}) from server `{URL_1_NAME}` differs from {term_of_comparison} (pt_safe={feedback_dict2['pt_safe']})\")\n",
    "        if TAL_feedback_dict1['pt_maybe'] != feedback_dict2['pt_maybe']:\n",
    "            print(f\"\\n\\nWARNING: the TALight opinion (pt_maybe={TAL_feedback_dict1['pt_maybe']}) from server `{URL_1_NAME}` differs from {term_of_comparison} (pt_maybe={feedback_dict2['pt_maybe']})\")\n",
    "        if TAL_feedback_dict1['pt_out'] != feedback_dict2['pt_out']:\n",
    "            print(f\"\\n\\nWARNING: the TALight opinion (pt_out={TAL_feedback_dict1['pt_out']}) from server `{URL_1_NAME}` differs from {term_of_comparison} (pt_out={feedback_dict2['pt_out']})\")\n",
    "\n",
    "\n",
    "def verify_submission(TALight_problem_name:str,checkers:List, task_dict:Dict, input_data_assigned:Dict, long_answer_dict:Dict):\n",
    "    #print(\"\\n\\n\\n\" f\"task_dict={task_dict}\" \"\\n\\n\\n\", file=stderr)\n",
    "    task_dict['with_positive_enforcement'] = '1' if 'positive_enforcements' in settings['FEEDBACKS_TO_STUDENT'] else '0'\n",
    "    task_dict['with_notes'] = '1' if 'cautious_notes' in settings['FEEDBACKS_TO_STUDENT'] else '0'\n",
    "    task_dict['as_yaml_with_points'] = '1'\n",
    "    task_dict['color_implementation'] = 'html'\n",
    "    task_dict['with_output_files'] = '0'\n",
    "    if settings['CHECKERS_INVOLVED'] == 'ONLY_THE_ONE_EMBEDDED_IN_NOTEBOOK':\n",
    "        checkers=['embedded_in_notebook']\n",
    "    elif settings['CHECKERS_INVOLVED'] == 'ONLY_THE_TALIGHT_ONES':\n",
    "        checkers=['TALight']\n",
    "    elif settings['CHECKERS_INVOLVED'] == 'BOTH':\n",
    "        checkers=['TALight', 'embedded_in_notebook']\n",
    "    elif settings['CHECKERS_INVOLVED'] == 'ONLY_FIRST_IN_LIST_FROM_YAML_LONGFILE':\n",
    "        checkers=checkers[0]\n",
    "    elif settings['CHECKERS_INVOLVED'] != 'THOSE_IN_LIST_FROM_YAML_LONGFILE':\n",
    "        print(f\"Unknown option `{settings['CHECKERS_INVOLVED']}` for the setting `CHECKERS_INVOLVED`\")\n",
    "        return\n",
    "    if 'TALight' in checkers:\n",
    "        if len(settings['RTAL_URLS']) == 0:\n",
    "            print(\"Invalid Settings File: `CHECKERS_INVOLVED`=`{settings['CHECKERS_INVOLVED']}` is in contraddiction with `RTAL_URLS` set to the empty list\")\n",
    "            return\n",
    "        answer_dict = { val[1]:val[0] for val in long_answer_dict.values() }\n",
    "        alias_dict = { val[1]:key for key, val in long_answer_dict.items() }\n",
    "        rtalargs_dict = task_dict\n",
    "        rtalargs_dict['input_data_assigned'] = input_data_assigned\n",
    "        rtalargs_dict['alias_dict'] = alias_dict\n",
    "        rtalargs_dict['answer_dict'] = answer_dict\n",
    "        URL_NAME = settings['RTAL_URLS'][0]\n",
    "        URL = settings[URL_NAME]\n",
    "        TAL_feedback_dict = rtal_connect(URL, TALight_problem_name, 'check', rtalargs_dict=rtalargs_dict, output_files_local_folder='output_files_TALight', rtaltoken=RTAL_PRIVATE_ACCESS_TOKEN)\n",
    "        monitor_what_submitted_to_rtald(TALight_problem_name,rtalargs_dict)\n",
    "        print(f\"Here is what the TALight checker thinks (and has stored on the server `{URL_NAME}`) about your submission:\")\n",
    "        if TAL_feedback_dict != None:\n",
    "            display(TAL_feedback_dict['feedback_string'])\n",
    "        elif 'signal_when_no_feedback_has_been_produced' in setting['RTAL_INTERFACE_VERBOSITY']:\n",
    "            print(\"The Feedback received from the server is EMPTY\")\n",
    "        for next_URL_NAME in settings['RTAL_URLS'][1:]:\n",
    "            URL = settings[next_URL_NAME]\n",
    "            print(f\"Next we pose the very same request also to the server `{next_URL_NAME}`:\")\n",
    "            next_TAL_feedback_dict = rtal_connect(URL, TALight_problem_name, 'check', rtalargs_dict=rtalargs_dict, output_files_local_folder='output_files_TALight', rtaltoken=RTAL_PRIVATE_ACCESS_TOKEN)\n",
    "            print(f\"Here is what the TALight checker thinks (and has stored on the server `{next_URL_NAME}`) about your submission:\")\n",
    "            if next_TAL_feedback_dict != None:\n",
    "                display(next_TAL_feedback_dict['feedback_string'])\n",
    "                check_checkers_agreement(URL_NAME,TAL_feedback_dict,next_TAL_feedback_dict,next_URL_NAME)\n",
    "            if next_TAL_feedback_dict != None:\n",
    "                display(TAL_feedback_dict['feedback_string'])\n",
    "            elif 'signal_when_no_feedback_has_been_produced' in setting['RTAL_INTERFACE_VERBOSITY']:\n",
    "                print(\"The Feedback received from the server is EMPTY\")\n",
    "\n",
    "    if 'embedded_in_notebook' in checkers:\n",
    "        print(\"Here is an independent evaluation (just in case the TALight server is down or unreachable):\")\n",
    "        SEF = std_eval_feedback(task_dict)\n",
    "        KingArthur = verify_submission_problem_specific(SEF, input_data_assigned, long_answer_dict)\n",
    "        embedded_feedback_dict = KingArthur.verify_submission(SEF)\n",
    "        #print(f\"feedback_dict={feedback_dict}\", file=stderr)\n",
    "        display(HTML(embedded_feedback_dict['feedback_string']))\n",
    "        if 'TALight' in checkers and settings['CHECK_AGREEMENT_OF_CHECKERS']:\n",
    "            check_checkers_agreement(URL_NAME,TAL_feedback_dict,embedded_feedback_dict)\n",
    "verify_submission(TALight_problem_name='RO_knapsack',checkers=['TALight', 'embedded_in_notebook'],task_dict={'task': 4, 'pt_tot': 40, 'pt_formato_OK': 0, 'pt_feasibility_OK': 1, 'pt_consistency_OK': 0}, input_data_assigned={'labels':['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'L', 'M', 'N'],'costs':[15, 16, 17, 11, 13, 5, 7, 3, 1, 12, 9, 7],'vals':[50, 52, 54, 40, 45, 17, 18, 7, 8, 42, 30, 22],'Knapsack_Capacity':28,'forced_out':[],'forced_in':[],'partialDPtable':[],}, long_answer_dict={'opt_sol':(opt_sol4,'opt_sol4'),'opt_val':(opt_val4,'opt_val4'),})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f25d5c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hide_input": true,
    "init_cell": true,
    "tags": [
     "noexport"
    ]
   },
   "outputs": [],
   "source": [
    "loader_main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c0f526",
   "metadata": {
    "deletable": false,
    "editable": true,
    "hide_input": false,
    "tags": [
     "runcell",
     "noexport"
    ]
   },
   "source": [
    "__Richiesta 5 [40 punti]__: Fornire una soluzione ottima se <b>36 è la capienza dello zaino</b> da non superarsi ma assumendo di <b>non poter prendere</b> nessuno degli elementi in ['E']."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37889854",
   "metadata": {
    "deletable": false,
    "editable": true,
    "hide_input": false,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "#Inserisci in `opt_sol5` la lista degli oggetti da prendere (esempio: ['E', 'N', 'M', 'L', 'I', 'H'])\n",
    "opt_sol5=[]\n",
    "#Immetti in `opt_val5` il valore della soluzione introdotta (un intero, la somma dei valori degli oggetti presi)\n",
    "opt_val5=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d21cdc4",
   "metadata": {
    "deletable": false,
    "editable": true,
    "hide_input": false,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "needed_varnames = ['opt_sol5', 'opt_val5', 'DPtable5', 'forced_out5', 'forced_in5']\n",
    "for varname in needed_varnames:\n",
    "    if varname not in locals():\n",
    "        exec(f'{varname} = None')\n",
    "# launch all requests for evaluation of the answer objects and get the feedback\n",
    "\n",
    "def check_checkers_agreement(URL_1_NAME, TAL_feedback_dict1, feedback_dict2, URL_2_NAME=None):\n",
    "    if URL_2_NAME is None:\n",
    "        term_of_comparison = \"that of the local King Arthur placed in the notebook\"\n",
    "    else:\n",
    "        term_of_comparison = \"that from server {URL_2_NAME}\"\n",
    "    if TAL_feedback_dict1 != None and feedback_dict2 != None and 'pt_safe' in TAL_feedback_dict1 and 'pt_maybe' in TAL_feedback_dict1 and 'pt_out' in TAL_feedback_dict1 and 'pt_safe' in feedback_dict2:\n",
    "        if TAL_feedback_dict1['pt_safe'] != feedback_dict2['pt_safe']:\n",
    "            print(f\"\\n\\nWARNING: the TALight opinion (pt_safe={TAL_feedback_dict1['pt_safe']}) from server `{URL_1_NAME}` differs from {term_of_comparison} (pt_safe={feedback_dict2['pt_safe']})\")\n",
    "        if TAL_feedback_dict1['pt_maybe'] != feedback_dict2['pt_maybe']:\n",
    "            print(f\"\\n\\nWARNING: the TALight opinion (pt_maybe={TAL_feedback_dict1['pt_maybe']}) from server `{URL_1_NAME}` differs from {term_of_comparison} (pt_maybe={feedback_dict2['pt_maybe']})\")\n",
    "        if TAL_feedback_dict1['pt_out'] != feedback_dict2['pt_out']:\n",
    "            print(f\"\\n\\nWARNING: the TALight opinion (pt_out={TAL_feedback_dict1['pt_out']}) from server `{URL_1_NAME}` differs from {term_of_comparison} (pt_out={feedback_dict2['pt_out']})\")\n",
    "\n",
    "\n",
    "def verify_submission(TALight_problem_name:str,checkers:List, task_dict:Dict, input_data_assigned:Dict, long_answer_dict:Dict):\n",
    "    #print(\"\\n\\n\\n\" f\"task_dict={task_dict}\" \"\\n\\n\\n\", file=stderr)\n",
    "    task_dict['with_positive_enforcement'] = '1' if 'positive_enforcements' in settings['FEEDBACKS_TO_STUDENT'] else '0'\n",
    "    task_dict['with_notes'] = '1' if 'cautious_notes' in settings['FEEDBACKS_TO_STUDENT'] else '0'\n",
    "    task_dict['as_yaml_with_points'] = '1'\n",
    "    task_dict['color_implementation'] = 'html'\n",
    "    task_dict['with_output_files'] = '0'\n",
    "    if settings['CHECKERS_INVOLVED'] == 'ONLY_THE_ONE_EMBEDDED_IN_NOTEBOOK':\n",
    "        checkers=['embedded_in_notebook']\n",
    "    elif settings['CHECKERS_INVOLVED'] == 'ONLY_THE_TALIGHT_ONES':\n",
    "        checkers=['TALight']\n",
    "    elif settings['CHECKERS_INVOLVED'] == 'BOTH':\n",
    "        checkers=['TALight', 'embedded_in_notebook']\n",
    "    elif settings['CHECKERS_INVOLVED'] == 'ONLY_FIRST_IN_LIST_FROM_YAML_LONGFILE':\n",
    "        checkers=checkers[0]\n",
    "    elif settings['CHECKERS_INVOLVED'] != 'THOSE_IN_LIST_FROM_YAML_LONGFILE':\n",
    "        print(f\"Unknown option `{settings['CHECKERS_INVOLVED']}` for the setting `CHECKERS_INVOLVED`\")\n",
    "        return\n",
    "    if 'TALight' in checkers:\n",
    "        if len(settings['RTAL_URLS']) == 0:\n",
    "            print(\"Invalid Settings File: `CHECKERS_INVOLVED`=`{settings['CHECKERS_INVOLVED']}` is in contraddiction with `RTAL_URLS` set to the empty list\")\n",
    "            return\n",
    "        answer_dict = { val[1]:val[0] for val in long_answer_dict.values() }\n",
    "        alias_dict = { val[1]:key for key, val in long_answer_dict.items() }\n",
    "        rtalargs_dict = task_dict\n",
    "        rtalargs_dict['input_data_assigned'] = input_data_assigned\n",
    "        rtalargs_dict['alias_dict'] = alias_dict\n",
    "        rtalargs_dict['answer_dict'] = answer_dict\n",
    "        URL_NAME = settings['RTAL_URLS'][0]\n",
    "        URL = settings[URL_NAME]\n",
    "        TAL_feedback_dict = rtal_connect(URL, TALight_problem_name, 'check', rtalargs_dict=rtalargs_dict, output_files_local_folder='output_files_TALight', rtaltoken=RTAL_PRIVATE_ACCESS_TOKEN)\n",
    "        monitor_what_submitted_to_rtald(TALight_problem_name,rtalargs_dict)\n",
    "        print(f\"Here is what the TALight checker thinks (and has stored on the server `{URL_NAME}`) about your submission:\")\n",
    "        if TAL_feedback_dict != None:\n",
    "            display(TAL_feedback_dict['feedback_string'])\n",
    "        elif 'signal_when_no_feedback_has_been_produced' in setting['RTAL_INTERFACE_VERBOSITY']:\n",
    "            print(\"The Feedback received from the server is EMPTY\")\n",
    "        for next_URL_NAME in settings['RTAL_URLS'][1:]:\n",
    "            URL = settings[next_URL_NAME]\n",
    "            print(f\"Next we pose the very same request also to the server `{next_URL_NAME}`:\")\n",
    "            next_TAL_feedback_dict = rtal_connect(URL, TALight_problem_name, 'check', rtalargs_dict=rtalargs_dict, output_files_local_folder='output_files_TALight', rtaltoken=RTAL_PRIVATE_ACCESS_TOKEN)\n",
    "            print(f\"Here is what the TALight checker thinks (and has stored on the server `{next_URL_NAME}`) about your submission:\")\n",
    "            if next_TAL_feedback_dict != None:\n",
    "                display(next_TAL_feedback_dict['feedback_string'])\n",
    "                check_checkers_agreement(URL_NAME,TAL_feedback_dict,next_TAL_feedback_dict,next_URL_NAME)\n",
    "            if next_TAL_feedback_dict != None:\n",
    "                display(TAL_feedback_dict['feedback_string'])\n",
    "            elif 'signal_when_no_feedback_has_been_produced' in setting['RTAL_INTERFACE_VERBOSITY']:\n",
    "                print(\"The Feedback received from the server is EMPTY\")\n",
    "\n",
    "    if 'embedded_in_notebook' in checkers:\n",
    "        print(\"Here is an independent evaluation (just in case the TALight server is down or unreachable):\")\n",
    "        SEF = std_eval_feedback(task_dict)\n",
    "        KingArthur = verify_submission_problem_specific(SEF, input_data_assigned, long_answer_dict)\n",
    "        embedded_feedback_dict = KingArthur.verify_submission(SEF)\n",
    "        #print(f\"feedback_dict={feedback_dict}\", file=stderr)\n",
    "        display(HTML(embedded_feedback_dict['feedback_string']))\n",
    "        if 'TALight' in checkers and settings['CHECK_AGREEMENT_OF_CHECKERS']:\n",
    "            check_checkers_agreement(URL_NAME,TAL_feedback_dict,embedded_feedback_dict)\n",
    "verify_submission(TALight_problem_name='RO_knapsack',checkers=['TALight', 'embedded_in_notebook'],task_dict={'task': 5, 'pt_tot': 40, 'pt_formato_OK': 0, 'pt_feasibility_OK': 1, 'pt_consistency_OK': 0}, input_data_assigned={'labels':['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'L', 'M', 'N'],'costs':[15, 16, 17, 11, 13, 5, 7, 3, 1, 12, 9, 7],'vals':[50, 52, 54, 40, 45, 17, 18, 7, 8, 42, 30, 22],'Knapsack_Capacity':36,'forced_out':['E'],'forced_in':[],'partialDPtable':[],}, long_answer_dict={'opt_sol':(opt_sol5,'opt_sol5'),'opt_val':(opt_val5,'opt_val5'),})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4bfed1",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hide_input": true,
    "init_cell": true,
    "tags": [
     "noexport"
    ]
   },
   "outputs": [],
   "source": [
    "loader_main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7109b871",
   "metadata": {
    "deletable": false,
    "editable": true,
    "hide_input": false,
    "tags": [
     "runcell",
     "noexport"
    ]
   },
   "source": [
    "__Richiesta 6 [40 punti]__: Fornire una soluzione ottima se <b>36 è la capienza dello zaino</b> da non superarsi ma assumendo di <b>non poter prendere</b> nessuno degli elementi in ['B', 'E']."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0815e19b",
   "metadata": {
    "deletable": false,
    "editable": true,
    "hide_input": false,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "#Inserisci in `opt_sol6` la lista degli oggetti da prendere (esempio: ['E', 'N', 'M', 'L', 'C', 'A'])\n",
    "opt_sol6=[]\n",
    "#Immetti in `opt_val6` il valore della soluzione introdotta (un intero, la somma dei valori degli oggetti presi)\n",
    "opt_val6=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de786fc",
   "metadata": {
    "deletable": false,
    "editable": true,
    "hide_input": false,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "needed_varnames = ['opt_sol6', 'opt_val6', 'DPtable6', 'forced_out6', 'forced_in6']\n",
    "for varname in needed_varnames:\n",
    "    if varname not in locals():\n",
    "        exec(f'{varname} = None')\n",
    "# launch all requests for evaluation of the answer objects and get the feedback\n",
    "\n",
    "def check_checkers_agreement(URL_1_NAME, TAL_feedback_dict1, feedback_dict2, URL_2_NAME=None):\n",
    "    if URL_2_NAME is None:\n",
    "        term_of_comparison = \"that of the local King Arthur placed in the notebook\"\n",
    "    else:\n",
    "        term_of_comparison = \"that from server {URL_2_NAME}\"\n",
    "    if TAL_feedback_dict1 != None and feedback_dict2 != None and 'pt_safe' in TAL_feedback_dict1 and 'pt_maybe' in TAL_feedback_dict1 and 'pt_out' in TAL_feedback_dict1 and 'pt_safe' in feedback_dict2:\n",
    "        if TAL_feedback_dict1['pt_safe'] != feedback_dict2['pt_safe']:\n",
    "            print(f\"\\n\\nWARNING: the TALight opinion (pt_safe={TAL_feedback_dict1['pt_safe']}) from server `{URL_1_NAME}` differs from {term_of_comparison} (pt_safe={feedback_dict2['pt_safe']})\")\n",
    "        if TAL_feedback_dict1['pt_maybe'] != feedback_dict2['pt_maybe']:\n",
    "            print(f\"\\n\\nWARNING: the TALight opinion (pt_maybe={TAL_feedback_dict1['pt_maybe']}) from server `{URL_1_NAME}` differs from {term_of_comparison} (pt_maybe={feedback_dict2['pt_maybe']})\")\n",
    "        if TAL_feedback_dict1['pt_out'] != feedback_dict2['pt_out']:\n",
    "            print(f\"\\n\\nWARNING: the TALight opinion (pt_out={TAL_feedback_dict1['pt_out']}) from server `{URL_1_NAME}` differs from {term_of_comparison} (pt_out={feedback_dict2['pt_out']})\")\n",
    "\n",
    "\n",
    "def verify_submission(TALight_problem_name:str,checkers:List, task_dict:Dict, input_data_assigned:Dict, long_answer_dict:Dict):\n",
    "    #print(\"\\n\\n\\n\" f\"task_dict={task_dict}\" \"\\n\\n\\n\", file=stderr)\n",
    "    task_dict['with_positive_enforcement'] = '1' if 'positive_enforcements' in settings['FEEDBACKS_TO_STUDENT'] else '0'\n",
    "    task_dict['with_notes'] = '1' if 'cautious_notes' in settings['FEEDBACKS_TO_STUDENT'] else '0'\n",
    "    task_dict['as_yaml_with_points'] = '1'\n",
    "    task_dict['color_implementation'] = 'html'\n",
    "    task_dict['with_output_files'] = '0'\n",
    "    if settings['CHECKERS_INVOLVED'] == 'ONLY_THE_ONE_EMBEDDED_IN_NOTEBOOK':\n",
    "        checkers=['embedded_in_notebook']\n",
    "    elif settings['CHECKERS_INVOLVED'] == 'ONLY_THE_TALIGHT_ONES':\n",
    "        checkers=['TALight']\n",
    "    elif settings['CHECKERS_INVOLVED'] == 'BOTH':\n",
    "        checkers=['TALight', 'embedded_in_notebook']\n",
    "    elif settings['CHECKERS_INVOLVED'] == 'ONLY_FIRST_IN_LIST_FROM_YAML_LONGFILE':\n",
    "        checkers=checkers[0]\n",
    "    elif settings['CHECKERS_INVOLVED'] != 'THOSE_IN_LIST_FROM_YAML_LONGFILE':\n",
    "        print(f\"Unknown option `{settings['CHECKERS_INVOLVED']}` for the setting `CHECKERS_INVOLVED`\")\n",
    "        return\n",
    "    if 'TALight' in checkers:\n",
    "        if len(settings['RTAL_URLS']) == 0:\n",
    "            print(\"Invalid Settings File: `CHECKERS_INVOLVED`=`{settings['CHECKERS_INVOLVED']}` is in contraddiction with `RTAL_URLS` set to the empty list\")\n",
    "            return\n",
    "        answer_dict = { val[1]:val[0] for val in long_answer_dict.values() }\n",
    "        alias_dict = { val[1]:key for key, val in long_answer_dict.items() }\n",
    "        rtalargs_dict = task_dict\n",
    "        rtalargs_dict['input_data_assigned'] = input_data_assigned\n",
    "        rtalargs_dict['alias_dict'] = alias_dict\n",
    "        rtalargs_dict['answer_dict'] = answer_dict\n",
    "        URL_NAME = settings['RTAL_URLS'][0]\n",
    "        URL = settings[URL_NAME]\n",
    "        TAL_feedback_dict = rtal_connect(URL, TALight_problem_name, 'check', rtalargs_dict=rtalargs_dict, output_files_local_folder='output_files_TALight', rtaltoken=RTAL_PRIVATE_ACCESS_TOKEN)\n",
    "        monitor_what_submitted_to_rtald(TALight_problem_name,rtalargs_dict)\n",
    "        print(f\"Here is what the TALight checker thinks (and has stored on the server `{URL_NAME}`) about your submission:\")\n",
    "        if TAL_feedback_dict != None:\n",
    "            display(TAL_feedback_dict['feedback_string'])\n",
    "        elif 'signal_when_no_feedback_has_been_produced' in setting['RTAL_INTERFACE_VERBOSITY']:\n",
    "            print(\"The Feedback received from the server is EMPTY\")\n",
    "        for next_URL_NAME in settings['RTAL_URLS'][1:]:\n",
    "            URL = settings[next_URL_NAME]\n",
    "            print(f\"Next we pose the very same request also to the server `{next_URL_NAME}`:\")\n",
    "            next_TAL_feedback_dict = rtal_connect(URL, TALight_problem_name, 'check', rtalargs_dict=rtalargs_dict, output_files_local_folder='output_files_TALight', rtaltoken=RTAL_PRIVATE_ACCESS_TOKEN)\n",
    "            print(f\"Here is what the TALight checker thinks (and has stored on the server `{next_URL_NAME}`) about your submission:\")\n",
    "            if next_TAL_feedback_dict != None:\n",
    "                display(next_TAL_feedback_dict['feedback_string'])\n",
    "                check_checkers_agreement(URL_NAME,TAL_feedback_dict,next_TAL_feedback_dict,next_URL_NAME)\n",
    "            if next_TAL_feedback_dict != None:\n",
    "                display(TAL_feedback_dict['feedback_string'])\n",
    "            elif 'signal_when_no_feedback_has_been_produced' in setting['RTAL_INTERFACE_VERBOSITY']:\n",
    "                print(\"The Feedback received from the server is EMPTY\")\n",
    "\n",
    "    if 'embedded_in_notebook' in checkers:\n",
    "        print(\"Here is an independent evaluation (just in case the TALight server is down or unreachable):\")\n",
    "        SEF = std_eval_feedback(task_dict)\n",
    "        KingArthur = verify_submission_problem_specific(SEF, input_data_assigned, long_answer_dict)\n",
    "        embedded_feedback_dict = KingArthur.verify_submission(SEF)\n",
    "        #print(f\"feedback_dict={feedback_dict}\", file=stderr)\n",
    "        display(HTML(embedded_feedback_dict['feedback_string']))\n",
    "        if 'TALight' in checkers and settings['CHECK_AGREEMENT_OF_CHECKERS']:\n",
    "            check_checkers_agreement(URL_NAME,TAL_feedback_dict,embedded_feedback_dict)\n",
    "verify_submission(TALight_problem_name='RO_knapsack',checkers=['TALight', 'embedded_in_notebook'],task_dict={'task': 6, 'pt_tot': 40, 'pt_formato_OK': 0, 'pt_feasibility_OK': 1, 'pt_consistency_OK': 0}, input_data_assigned={'labels':['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'L', 'M', 'N'],'costs':[15, 16, 17, 11, 13, 5, 7, 3, 1, 12, 9, 7],'vals':[50, 52, 54, 40, 45, 17, 18, 7, 8, 42, 30, 22],'Knapsack_Capacity':36,'forced_out':['B', 'E'],'forced_in':[],'partialDPtable':[],}, long_answer_dict={'opt_sol':(opt_sol6,'opt_sol6'),'opt_val':(opt_val6,'opt_val6'),})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4d813c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hide_input": true,
    "init_cell": true,
    "tags": [
     "noexport"
    ]
   },
   "outputs": [],
   "source": [
    "loader_main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62636efe",
   "metadata": {
    "deletable": false,
    "editable": true,
    "hide_input": false,
    "tags": [
     "runcell",
     "noexport"
    ]
   },
   "source": [
    "__Richiesta 7 [40 punti]__: Fornire una soluzione ottima se <b>34 è la capienza dello zaino</b> da non superarsi ma assumendo di <b>non poter prendere</b> nessuno degli elementi in ['B', 'E', 'F']."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4045a610",
   "metadata": {
    "deletable": false,
    "editable": true,
    "hide_input": false,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "#Inserisci in `opt_sol7` la lista degli oggetti da prendere (esempio: ['F', 'N', 'M', 'L', 'I', 'H', 'D', 'C'])\n",
    "opt_sol7=[]\n",
    "#Immetti in `opt_val7` il valore della soluzione introdotta (un intero, la somma dei valori degli oggetti presi)\n",
    "opt_val7=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3087f25",
   "metadata": {
    "deletable": false,
    "editable": true,
    "hide_input": false,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "needed_varnames = ['opt_sol7', 'opt_val7', 'DPtable7', 'forced_out7', 'forced_in7']\n",
    "for varname in needed_varnames:\n",
    "    if varname not in locals():\n",
    "        exec(f'{varname} = None')\n",
    "# launch all requests for evaluation of the answer objects and get the feedback\n",
    "\n",
    "def check_checkers_agreement(URL_1_NAME, TAL_feedback_dict1, feedback_dict2, URL_2_NAME=None):\n",
    "    if URL_2_NAME is None:\n",
    "        term_of_comparison = \"that of the local King Arthur placed in the notebook\"\n",
    "    else:\n",
    "        term_of_comparison = \"that from server {URL_2_NAME}\"\n",
    "    if TAL_feedback_dict1 != None and feedback_dict2 != None and 'pt_safe' in TAL_feedback_dict1 and 'pt_maybe' in TAL_feedback_dict1 and 'pt_out' in TAL_feedback_dict1 and 'pt_safe' in feedback_dict2:\n",
    "        if TAL_feedback_dict1['pt_safe'] != feedback_dict2['pt_safe']:\n",
    "            print(f\"\\n\\nWARNING: the TALight opinion (pt_safe={TAL_feedback_dict1['pt_safe']}) from server `{URL_1_NAME}` differs from {term_of_comparison} (pt_safe={feedback_dict2['pt_safe']})\")\n",
    "        if TAL_feedback_dict1['pt_maybe'] != feedback_dict2['pt_maybe']:\n",
    "            print(f\"\\n\\nWARNING: the TALight opinion (pt_maybe={TAL_feedback_dict1['pt_maybe']}) from server `{URL_1_NAME}` differs from {term_of_comparison} (pt_maybe={feedback_dict2['pt_maybe']})\")\n",
    "        if TAL_feedback_dict1['pt_out'] != feedback_dict2['pt_out']:\n",
    "            print(f\"\\n\\nWARNING: the TALight opinion (pt_out={TAL_feedback_dict1['pt_out']}) from server `{URL_1_NAME}` differs from {term_of_comparison} (pt_out={feedback_dict2['pt_out']})\")\n",
    "\n",
    "\n",
    "def verify_submission(TALight_problem_name:str,checkers:List, task_dict:Dict, input_data_assigned:Dict, long_answer_dict:Dict):\n",
    "    #print(\"\\n\\n\\n\" f\"task_dict={task_dict}\" \"\\n\\n\\n\", file=stderr)\n",
    "    task_dict['with_positive_enforcement'] = '1' if 'positive_enforcements' in settings['FEEDBACKS_TO_STUDENT'] else '0'\n",
    "    task_dict['with_notes'] = '1' if 'cautious_notes' in settings['FEEDBACKS_TO_STUDENT'] else '0'\n",
    "    task_dict['as_yaml_with_points'] = '1'\n",
    "    task_dict['color_implementation'] = 'html'\n",
    "    task_dict['with_output_files'] = '0'\n",
    "    if settings['CHECKERS_INVOLVED'] == 'ONLY_THE_ONE_EMBEDDED_IN_NOTEBOOK':\n",
    "        checkers=['embedded_in_notebook']\n",
    "    elif settings['CHECKERS_INVOLVED'] == 'ONLY_THE_TALIGHT_ONES':\n",
    "        checkers=['TALight']\n",
    "    elif settings['CHECKERS_INVOLVED'] == 'BOTH':\n",
    "        checkers=['TALight', 'embedded_in_notebook']\n",
    "    elif settings['CHECKERS_INVOLVED'] == 'ONLY_FIRST_IN_LIST_FROM_YAML_LONGFILE':\n",
    "        checkers=checkers[0]\n",
    "    elif settings['CHECKERS_INVOLVED'] != 'THOSE_IN_LIST_FROM_YAML_LONGFILE':\n",
    "        print(f\"Unknown option `{settings['CHECKERS_INVOLVED']}` for the setting `CHECKERS_INVOLVED`\")\n",
    "        return\n",
    "    if 'TALight' in checkers:\n",
    "        if len(settings['RTAL_URLS']) == 0:\n",
    "            print(\"Invalid Settings File: `CHECKERS_INVOLVED`=`{settings['CHECKERS_INVOLVED']}` is in contraddiction with `RTAL_URLS` set to the empty list\")\n",
    "            return\n",
    "        answer_dict = { val[1]:val[0] for val in long_answer_dict.values() }\n",
    "        alias_dict = { val[1]:key for key, val in long_answer_dict.items() }\n",
    "        rtalargs_dict = task_dict\n",
    "        rtalargs_dict['input_data_assigned'] = input_data_assigned\n",
    "        rtalargs_dict['alias_dict'] = alias_dict\n",
    "        rtalargs_dict['answer_dict'] = answer_dict\n",
    "        URL_NAME = settings['RTAL_URLS'][0]\n",
    "        URL = settings[URL_NAME]\n",
    "        TAL_feedback_dict = rtal_connect(URL, TALight_problem_name, 'check', rtalargs_dict=rtalargs_dict, output_files_local_folder='output_files_TALight', rtaltoken=RTAL_PRIVATE_ACCESS_TOKEN)\n",
    "        monitor_what_submitted_to_rtald(TALight_problem_name,rtalargs_dict)\n",
    "        print(f\"Here is what the TALight checker thinks (and has stored on the server `{URL_NAME}`) about your submission:\")\n",
    "        if TAL_feedback_dict != None:\n",
    "            display(TAL_feedback_dict['feedback_string'])\n",
    "        elif 'signal_when_no_feedback_has_been_produced' in setting['RTAL_INTERFACE_VERBOSITY']:\n",
    "            print(\"The Feedback received from the server is EMPTY\")\n",
    "        for next_URL_NAME in settings['RTAL_URLS'][1:]:\n",
    "            URL = settings[next_URL_NAME]\n",
    "            print(f\"Next we pose the very same request also to the server `{next_URL_NAME}`:\")\n",
    "            next_TAL_feedback_dict = rtal_connect(URL, TALight_problem_name, 'check', rtalargs_dict=rtalargs_dict, output_files_local_folder='output_files_TALight', rtaltoken=RTAL_PRIVATE_ACCESS_TOKEN)\n",
    "            print(f\"Here is what the TALight checker thinks (and has stored on the server `{next_URL_NAME}`) about your submission:\")\n",
    "            if next_TAL_feedback_dict != None:\n",
    "                display(next_TAL_feedback_dict['feedback_string'])\n",
    "                check_checkers_agreement(URL_NAME,TAL_feedback_dict,next_TAL_feedback_dict,next_URL_NAME)\n",
    "            if next_TAL_feedback_dict != None:\n",
    "                display(TAL_feedback_dict['feedback_string'])\n",
    "            elif 'signal_when_no_feedback_has_been_produced' in setting['RTAL_INTERFACE_VERBOSITY']:\n",
    "                print(\"The Feedback received from the server is EMPTY\")\n",
    "\n",
    "    if 'embedded_in_notebook' in checkers:\n",
    "        print(\"Here is an independent evaluation (just in case the TALight server is down or unreachable):\")\n",
    "        SEF = std_eval_feedback(task_dict)\n",
    "        KingArthur = verify_submission_problem_specific(SEF, input_data_assigned, long_answer_dict)\n",
    "        embedded_feedback_dict = KingArthur.verify_submission(SEF)\n",
    "        #print(f\"feedback_dict={feedback_dict}\", file=stderr)\n",
    "        display(HTML(embedded_feedback_dict['feedback_string']))\n",
    "        if 'TALight' in checkers and settings['CHECK_AGREEMENT_OF_CHECKERS']:\n",
    "            check_checkers_agreement(URL_NAME,TAL_feedback_dict,embedded_feedback_dict)\n",
    "verify_submission(TALight_problem_name='RO_knapsack',checkers=['TALight', 'embedded_in_notebook'],task_dict={'task': 7, 'pt_tot': 40, 'pt_formato_OK': 0, 'pt_feasibility_OK': 1, 'pt_consistency_OK': 0}, input_data_assigned={'labels':['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'L', 'M', 'N'],'costs':[15, 16, 17, 11, 13, 5, 7, 3, 1, 12, 9, 7],'vals':[50, 52, 54, 40, 45, 17, 18, 7, 8, 42, 30, 22],'Knapsack_Capacity':34,'forced_out':['B', 'E', 'F'],'forced_in':[],'partialDPtable':[],}, long_answer_dict={'opt_sol':(opt_sol7,'opt_sol7'),'opt_val':(opt_val7,'opt_val7'),})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b144553f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hide_input": true,
    "init_cell": true,
    "tags": [
     "noexport"
    ]
   },
   "outputs": [],
   "source": [
    "loader_main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611ea904",
   "metadata": {
    "deletable": false,
    "editable": true,
    "hide_input": false,
    "tags": [
     "runcell",
     "noexport"
    ]
   },
   "source": [
    "__Richiesta 8 [40 punti]__: Fornire una soluzione ottima se <b>34 è la capienza dello zaino</b> da non superarsi ma assumendo di <b>dover prendere</b> tutti gli elementi in ['B', 'E']."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71a5f20",
   "metadata": {
    "deletable": false,
    "editable": true,
    "hide_input": false,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "#Inserisci in `opt_val8` il massimo valore possibile per una soluzione ammissibile (un intero, la somma dei valori degli oggetti presi)\n",
    "opt_val8=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4601f9f2",
   "metadata": {
    "deletable": false,
    "editable": true,
    "hide_input": false,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "needed_varnames = ['opt_sol8', 'opt_val8', 'DPtable8', 'forced_out8', 'forced_in8']\n",
    "for varname in needed_varnames:\n",
    "    if varname not in locals():\n",
    "        exec(f'{varname} = None')\n",
    "# launch all requests for evaluation of the answer objects and get the feedback\n",
    "\n",
    "def check_checkers_agreement(URL_1_NAME, TAL_feedback_dict1, feedback_dict2, URL_2_NAME=None):\n",
    "    if URL_2_NAME is None:\n",
    "        term_of_comparison = \"that of the local King Arthur placed in the notebook\"\n",
    "    else:\n",
    "        term_of_comparison = \"that from server {URL_2_NAME}\"\n",
    "    if TAL_feedback_dict1 != None and feedback_dict2 != None and 'pt_safe' in TAL_feedback_dict1 and 'pt_maybe' in TAL_feedback_dict1 and 'pt_out' in TAL_feedback_dict1 and 'pt_safe' in feedback_dict2:\n",
    "        if TAL_feedback_dict1['pt_safe'] != feedback_dict2['pt_safe']:\n",
    "            print(f\"\\n\\nWARNING: the TALight opinion (pt_safe={TAL_feedback_dict1['pt_safe']}) from server `{URL_1_NAME}` differs from {term_of_comparison} (pt_safe={feedback_dict2['pt_safe']})\")\n",
    "        if TAL_feedback_dict1['pt_maybe'] != feedback_dict2['pt_maybe']:\n",
    "            print(f\"\\n\\nWARNING: the TALight opinion (pt_maybe={TAL_feedback_dict1['pt_maybe']}) from server `{URL_1_NAME}` differs from {term_of_comparison} (pt_maybe={feedback_dict2['pt_maybe']})\")\n",
    "        if TAL_feedback_dict1['pt_out'] != feedback_dict2['pt_out']:\n",
    "            print(f\"\\n\\nWARNING: the TALight opinion (pt_out={TAL_feedback_dict1['pt_out']}) from server `{URL_1_NAME}` differs from {term_of_comparison} (pt_out={feedback_dict2['pt_out']})\")\n",
    "\n",
    "\n",
    "def verify_submission(TALight_problem_name:str,checkers:List, task_dict:Dict, input_data_assigned:Dict, long_answer_dict:Dict):\n",
    "    #print(\"\\n\\n\\n\" f\"task_dict={task_dict}\" \"\\n\\n\\n\", file=stderr)\n",
    "    task_dict['with_positive_enforcement'] = '1' if 'positive_enforcements' in settings['FEEDBACKS_TO_STUDENT'] else '0'\n",
    "    task_dict['with_notes'] = '1' if 'cautious_notes' in settings['FEEDBACKS_TO_STUDENT'] else '0'\n",
    "    task_dict['as_yaml_with_points'] = '1'\n",
    "    task_dict['color_implementation'] = 'html'\n",
    "    task_dict['with_output_files'] = '0'\n",
    "    if settings['CHECKERS_INVOLVED'] == 'ONLY_THE_ONE_EMBEDDED_IN_NOTEBOOK':\n",
    "        checkers=['embedded_in_notebook']\n",
    "    elif settings['CHECKERS_INVOLVED'] == 'ONLY_THE_TALIGHT_ONES':\n",
    "        checkers=['TALight']\n",
    "    elif settings['CHECKERS_INVOLVED'] == 'BOTH':\n",
    "        checkers=['TALight', 'embedded_in_notebook']\n",
    "    elif settings['CHECKERS_INVOLVED'] == 'ONLY_FIRST_IN_LIST_FROM_YAML_LONGFILE':\n",
    "        checkers=checkers[0]\n",
    "    elif settings['CHECKERS_INVOLVED'] != 'THOSE_IN_LIST_FROM_YAML_LONGFILE':\n",
    "        print(f\"Unknown option `{settings['CHECKERS_INVOLVED']}` for the setting `CHECKERS_INVOLVED`\")\n",
    "        return\n",
    "    if 'TALight' in checkers:\n",
    "        if len(settings['RTAL_URLS']) == 0:\n",
    "            print(\"Invalid Settings File: `CHECKERS_INVOLVED`=`{settings['CHECKERS_INVOLVED']}` is in contraddiction with `RTAL_URLS` set to the empty list\")\n",
    "            return\n",
    "        answer_dict = { val[1]:val[0] for val in long_answer_dict.values() }\n",
    "        alias_dict = { val[1]:key for key, val in long_answer_dict.items() }\n",
    "        rtalargs_dict = task_dict\n",
    "        rtalargs_dict['input_data_assigned'] = input_data_assigned\n",
    "        rtalargs_dict['alias_dict'] = alias_dict\n",
    "        rtalargs_dict['answer_dict'] = answer_dict\n",
    "        URL_NAME = settings['RTAL_URLS'][0]\n",
    "        URL = settings[URL_NAME]\n",
    "        TAL_feedback_dict = rtal_connect(URL, TALight_problem_name, 'check', rtalargs_dict=rtalargs_dict, output_files_local_folder='output_files_TALight', rtaltoken=RTAL_PRIVATE_ACCESS_TOKEN)\n",
    "        monitor_what_submitted_to_rtald(TALight_problem_name,rtalargs_dict)\n",
    "        print(f\"Here is what the TALight checker thinks (and has stored on the server `{URL_NAME}`) about your submission:\")\n",
    "        if TAL_feedback_dict != None:\n",
    "            display(TAL_feedback_dict['feedback_string'])\n",
    "        elif 'signal_when_no_feedback_has_been_produced' in setting['RTAL_INTERFACE_VERBOSITY']:\n",
    "            print(\"The Feedback received from the server is EMPTY\")\n",
    "        for next_URL_NAME in settings['RTAL_URLS'][1:]:\n",
    "            URL = settings[next_URL_NAME]\n",
    "            print(f\"Next we pose the very same request also to the server `{next_URL_NAME}`:\")\n",
    "            next_TAL_feedback_dict = rtal_connect(URL, TALight_problem_name, 'check', rtalargs_dict=rtalargs_dict, output_files_local_folder='output_files_TALight', rtaltoken=RTAL_PRIVATE_ACCESS_TOKEN)\n",
    "            print(f\"Here is what the TALight checker thinks (and has stored on the server `{next_URL_NAME}`) about your submission:\")\n",
    "            if next_TAL_feedback_dict != None:\n",
    "                display(next_TAL_feedback_dict['feedback_string'])\n",
    "                check_checkers_agreement(URL_NAME,TAL_feedback_dict,next_TAL_feedback_dict,next_URL_NAME)\n",
    "            if next_TAL_feedback_dict != None:\n",
    "                display(TAL_feedback_dict['feedback_string'])\n",
    "            elif 'signal_when_no_feedback_has_been_produced' in setting['RTAL_INTERFACE_VERBOSITY']:\n",
    "                print(\"The Feedback received from the server is EMPTY\")\n",
    "\n",
    "    if 'embedded_in_notebook' in checkers:\n",
    "        print(\"Here is an independent evaluation (just in case the TALight server is down or unreachable):\")\n",
    "        SEF = std_eval_feedback(task_dict)\n",
    "        KingArthur = verify_submission_problem_specific(SEF, input_data_assigned, long_answer_dict)\n",
    "        embedded_feedback_dict = KingArthur.verify_submission(SEF)\n",
    "        #print(f\"feedback_dict={feedback_dict}\", file=stderr)\n",
    "        display(HTML(embedded_feedback_dict['feedback_string']))\n",
    "        if 'TALight' in checkers and settings['CHECK_AGREEMENT_OF_CHECKERS']:\n",
    "            check_checkers_agreement(URL_NAME,TAL_feedback_dict,embedded_feedback_dict)\n",
    "verify_submission(TALight_problem_name='RO_knapsack',checkers=['TALight', 'embedded_in_notebook'],task_dict={'task': 8, 'pt_tot': 40, 'pt_formato_OK': 0, 'pt_feasibility_OK': 1, 'pt_consistency_OK': 0}, input_data_assigned={'labels':['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'L', 'M', 'N'],'costs':[15, 16, 17, 11, 13, 5, 7, 3, 1, 12, 9, 7],'vals':[50, 52, 54, 40, 45, 17, 18, 7, 8, 42, 30, 22],'Knapsack_Capacity':34,'forced_out':[],'forced_in':['B', 'E'],'partialDPtable':[],}, long_answer_dict={'opt_val':(opt_val8,'opt_val8'),})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52ac105",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hide_input": true,
    "init_cell": true,
    "tags": [
     "noexport"
    ]
   },
   "outputs": [],
   "source": [
    "loader_main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8e2643",
   "metadata": {
    "deletable": false,
    "editable": true,
    "hide_input": false,
    "tags": [
     "runcell",
     "noexport"
    ]
   },
   "source": [
    "__Richiesta 9 [40 punti]__: Fornire una soluzione ottima se <b>34 è la capienza dello zaino</b> da non superarsi ma assumendo di <b>dover prendere tutti</b> gli elementi in ['B', 'F'] e <b>nessuno</b> di quelli in ['E']."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36372a04",
   "metadata": {
    "deletable": false,
    "editable": true,
    "hide_input": false,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "#Inserisci in `opt_sol9` la lista degli oggetti da prendere (esempio: ['F', 'E', 'N', 'M', 'L', 'G', 'C'])\n",
    "opt_sol9=[]\n",
    "#Immetti in `opt_val9` il valore della soluzione introdotta (un intero, la somma dei valori degli oggetti presi)\n",
    "opt_val9=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfeda843",
   "metadata": {
    "deletable": false,
    "editable": true,
    "hide_input": false,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "needed_varnames = ['opt_sol9', 'opt_val9', 'DPtable9', 'forced_out9', 'forced_in9']\n",
    "for varname in needed_varnames:\n",
    "    if varname not in locals():\n",
    "        exec(f'{varname} = None')\n",
    "# launch all requests for evaluation of the answer objects and get the feedback\n",
    "\n",
    "def check_checkers_agreement(URL_1_NAME, TAL_feedback_dict1, feedback_dict2, URL_2_NAME=None):\n",
    "    if URL_2_NAME is None:\n",
    "        term_of_comparison = \"that of the local King Arthur placed in the notebook\"\n",
    "    else:\n",
    "        term_of_comparison = \"that from server {URL_2_NAME}\"\n",
    "    if TAL_feedback_dict1 != None and feedback_dict2 != None and 'pt_safe' in TAL_feedback_dict1 and 'pt_maybe' in TAL_feedback_dict1 and 'pt_out' in TAL_feedback_dict1 and 'pt_safe' in feedback_dict2:\n",
    "        if TAL_feedback_dict1['pt_safe'] != feedback_dict2['pt_safe']:\n",
    "            print(f\"\\n\\nWARNING: the TALight opinion (pt_safe={TAL_feedback_dict1['pt_safe']}) from server `{URL_1_NAME}` differs from {term_of_comparison} (pt_safe={feedback_dict2['pt_safe']})\")\n",
    "        if TAL_feedback_dict1['pt_maybe'] != feedback_dict2['pt_maybe']:\n",
    "            print(f\"\\n\\nWARNING: the TALight opinion (pt_maybe={TAL_feedback_dict1['pt_maybe']}) from server `{URL_1_NAME}` differs from {term_of_comparison} (pt_maybe={feedback_dict2['pt_maybe']})\")\n",
    "        if TAL_feedback_dict1['pt_out'] != feedback_dict2['pt_out']:\n",
    "            print(f\"\\n\\nWARNING: the TALight opinion (pt_out={TAL_feedback_dict1['pt_out']}) from server `{URL_1_NAME}` differs from {term_of_comparison} (pt_out={feedback_dict2['pt_out']})\")\n",
    "\n",
    "\n",
    "def verify_submission(TALight_problem_name:str,checkers:List, task_dict:Dict, input_data_assigned:Dict, long_answer_dict:Dict):\n",
    "    #print(\"\\n\\n\\n\" f\"task_dict={task_dict}\" \"\\n\\n\\n\", file=stderr)\n",
    "    task_dict['with_positive_enforcement'] = '1' if 'positive_enforcements' in settings['FEEDBACKS_TO_STUDENT'] else '0'\n",
    "    task_dict['with_notes'] = '1' if 'cautious_notes' in settings['FEEDBACKS_TO_STUDENT'] else '0'\n",
    "    task_dict['as_yaml_with_points'] = '1'\n",
    "    task_dict['color_implementation'] = 'html'\n",
    "    task_dict['with_output_files'] = '0'\n",
    "    if settings['CHECKERS_INVOLVED'] == 'ONLY_THE_ONE_EMBEDDED_IN_NOTEBOOK':\n",
    "        checkers=['embedded_in_notebook']\n",
    "    elif settings['CHECKERS_INVOLVED'] == 'ONLY_THE_TALIGHT_ONES':\n",
    "        checkers=['TALight']\n",
    "    elif settings['CHECKERS_INVOLVED'] == 'BOTH':\n",
    "        checkers=['TALight', 'embedded_in_notebook']\n",
    "    elif settings['CHECKERS_INVOLVED'] == 'ONLY_FIRST_IN_LIST_FROM_YAML_LONGFILE':\n",
    "        checkers=checkers[0]\n",
    "    elif settings['CHECKERS_INVOLVED'] != 'THOSE_IN_LIST_FROM_YAML_LONGFILE':\n",
    "        print(f\"Unknown option `{settings['CHECKERS_INVOLVED']}` for the setting `CHECKERS_INVOLVED`\")\n",
    "        return\n",
    "    if 'TALight' in checkers:\n",
    "        if len(settings['RTAL_URLS']) == 0:\n",
    "            print(\"Invalid Settings File: `CHECKERS_INVOLVED`=`{settings['CHECKERS_INVOLVED']}` is in contraddiction with `RTAL_URLS` set to the empty list\")\n",
    "            return\n",
    "        answer_dict = { val[1]:val[0] for val in long_answer_dict.values() }\n",
    "        alias_dict = { val[1]:key for key, val in long_answer_dict.items() }\n",
    "        rtalargs_dict = task_dict\n",
    "        rtalargs_dict['input_data_assigned'] = input_data_assigned\n",
    "        rtalargs_dict['alias_dict'] = alias_dict\n",
    "        rtalargs_dict['answer_dict'] = answer_dict\n",
    "        URL_NAME = settings['RTAL_URLS'][0]\n",
    "        URL = settings[URL_NAME]\n",
    "        TAL_feedback_dict = rtal_connect(URL, TALight_problem_name, 'check', rtalargs_dict=rtalargs_dict, output_files_local_folder='output_files_TALight', rtaltoken=RTAL_PRIVATE_ACCESS_TOKEN)\n",
    "        monitor_what_submitted_to_rtald(TALight_problem_name,rtalargs_dict)\n",
    "        print(f\"Here is what the TALight checker thinks (and has stored on the server `{URL_NAME}`) about your submission:\")\n",
    "        if TAL_feedback_dict != None:\n",
    "            display(TAL_feedback_dict['feedback_string'])\n",
    "        elif 'signal_when_no_feedback_has_been_produced' in setting['RTAL_INTERFACE_VERBOSITY']:\n",
    "            print(\"The Feedback received from the server is EMPTY\")\n",
    "        for next_URL_NAME in settings['RTAL_URLS'][1:]:\n",
    "            URL = settings[next_URL_NAME]\n",
    "            print(f\"Next we pose the very same request also to the server `{next_URL_NAME}`:\")\n",
    "            next_TAL_feedback_dict = rtal_connect(URL, TALight_problem_name, 'check', rtalargs_dict=rtalargs_dict, output_files_local_folder='output_files_TALight', rtaltoken=RTAL_PRIVATE_ACCESS_TOKEN)\n",
    "            print(f\"Here is what the TALight checker thinks (and has stored on the server `{next_URL_NAME}`) about your submission:\")\n",
    "            if next_TAL_feedback_dict != None:\n",
    "                display(next_TAL_feedback_dict['feedback_string'])\n",
    "                check_checkers_agreement(URL_NAME,TAL_feedback_dict,next_TAL_feedback_dict,next_URL_NAME)\n",
    "            if next_TAL_feedback_dict != None:\n",
    "                display(TAL_feedback_dict['feedback_string'])\n",
    "            elif 'signal_when_no_feedback_has_been_produced' in setting['RTAL_INTERFACE_VERBOSITY']:\n",
    "                print(\"The Feedback received from the server is EMPTY\")\n",
    "\n",
    "    if 'embedded_in_notebook' in checkers:\n",
    "        print(\"Here is an independent evaluation (just in case the TALight server is down or unreachable):\")\n",
    "        SEF = std_eval_feedback(task_dict)\n",
    "        KingArthur = verify_submission_problem_specific(SEF, input_data_assigned, long_answer_dict)\n",
    "        embedded_feedback_dict = KingArthur.verify_submission(SEF)\n",
    "        #print(f\"feedback_dict={feedback_dict}\", file=stderr)\n",
    "        display(HTML(embedded_feedback_dict['feedback_string']))\n",
    "        if 'TALight' in checkers and settings['CHECK_AGREEMENT_OF_CHECKERS']:\n",
    "            check_checkers_agreement(URL_NAME,TAL_feedback_dict,embedded_feedback_dict)\n",
    "verify_submission(TALight_problem_name='RO_knapsack',checkers=['TALight', 'embedded_in_notebook'],task_dict={'task': 9, 'pt_tot': 40, 'pt_formato_OK': 0, 'pt_feasibility_OK': 1, 'pt_consistency_OK': 0}, input_data_assigned={'labels':['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'L', 'M', 'N'],'costs':[15, 16, 17, 11, 13, 5, 7, 3, 1, 12, 9, 7],'vals':[50, 52, 54, 40, 45, 17, 18, 7, 8, 42, 30, 22],'Knapsack_Capacity':34,'forced_out':['E'],'forced_in':['B', 'F'],'partialDPtable':[],}, long_answer_dict={'opt_sol':(opt_sol9,'opt_sol9'),'opt_val':(opt_val9,'opt_val9'),})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d098084e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hide_input": true,
    "init_cell": true,
    "tags": [
     "noexport"
    ]
   },
   "outputs": [],
   "source": [
    "loader_main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469a738a",
   "metadata": {
    "deletable": false,
    "editable": true,
    "hide_input": false,
    "tags": [
     "runcell",
     "noexport"
    ]
   },
   "source": [
    "__Richiesta 10 [40 punti]__: Fornire una soluzione ottima se <b>31 è la capienza dello zaino</b> da non superarsi ma assumendo di <b>dover prendere tutti</b> gli elementi in ['B', 'I'] e <b>nessuno</b> di quelli in ['F', 'E']."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a036fa70",
   "metadata": {
    "deletable": false,
    "editable": true,
    "hide_input": false,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "#Inserisci la tua risposta in forma di lista di oggetti da prendere (esempio: ['I', 'E', 'N', 'M', 'L', 'F', 'D'])\n",
    "opt_sol10=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ddc650",
   "metadata": {
    "deletable": false,
    "editable": true,
    "hide_input": false,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "needed_varnames = ['opt_sol10', 'opt_val10', 'DPtable10', 'forced_out10', 'forced_in10']\n",
    "for varname in needed_varnames:\n",
    "    if varname not in locals():\n",
    "        exec(f'{varname} = None')\n",
    "# launch all requests for evaluation of the answer objects and get the feedback\n",
    "\n",
    "def check_checkers_agreement(URL_1_NAME, TAL_feedback_dict1, feedback_dict2, URL_2_NAME=None):\n",
    "    if URL_2_NAME is None:\n",
    "        term_of_comparison = \"that of the local King Arthur placed in the notebook\"\n",
    "    else:\n",
    "        term_of_comparison = \"that from server {URL_2_NAME}\"\n",
    "    if TAL_feedback_dict1 != None and feedback_dict2 != None and 'pt_safe' in TAL_feedback_dict1 and 'pt_maybe' in TAL_feedback_dict1 and 'pt_out' in TAL_feedback_dict1 and 'pt_safe' in feedback_dict2:\n",
    "        if TAL_feedback_dict1['pt_safe'] != feedback_dict2['pt_safe']:\n",
    "            print(f\"\\n\\nWARNING: the TALight opinion (pt_safe={TAL_feedback_dict1['pt_safe']}) from server `{URL_1_NAME}` differs from {term_of_comparison} (pt_safe={feedback_dict2['pt_safe']})\")\n",
    "        if TAL_feedback_dict1['pt_maybe'] != feedback_dict2['pt_maybe']:\n",
    "            print(f\"\\n\\nWARNING: the TALight opinion (pt_maybe={TAL_feedback_dict1['pt_maybe']}) from server `{URL_1_NAME}` differs from {term_of_comparison} (pt_maybe={feedback_dict2['pt_maybe']})\")\n",
    "        if TAL_feedback_dict1['pt_out'] != feedback_dict2['pt_out']:\n",
    "            print(f\"\\n\\nWARNING: the TALight opinion (pt_out={TAL_feedback_dict1['pt_out']}) from server `{URL_1_NAME}` differs from {term_of_comparison} (pt_out={feedback_dict2['pt_out']})\")\n",
    "\n",
    "\n",
    "def verify_submission(TALight_problem_name:str,checkers:List, task_dict:Dict, input_data_assigned:Dict, long_answer_dict:Dict):\n",
    "    #print(\"\\n\\n\\n\" f\"task_dict={task_dict}\" \"\\n\\n\\n\", file=stderr)\n",
    "    task_dict['with_positive_enforcement'] = '1' if 'positive_enforcements' in settings['FEEDBACKS_TO_STUDENT'] else '0'\n",
    "    task_dict['with_notes'] = '1' if 'cautious_notes' in settings['FEEDBACKS_TO_STUDENT'] else '0'\n",
    "    task_dict['as_yaml_with_points'] = '1'\n",
    "    task_dict['color_implementation'] = 'html'\n",
    "    task_dict['with_output_files'] = '0'\n",
    "    if settings['CHECKERS_INVOLVED'] == 'ONLY_THE_ONE_EMBEDDED_IN_NOTEBOOK':\n",
    "        checkers=['embedded_in_notebook']\n",
    "    elif settings['CHECKERS_INVOLVED'] == 'ONLY_THE_TALIGHT_ONES':\n",
    "        checkers=['TALight']\n",
    "    elif settings['CHECKERS_INVOLVED'] == 'BOTH':\n",
    "        checkers=['TALight', 'embedded_in_notebook']\n",
    "    elif settings['CHECKERS_INVOLVED'] == 'ONLY_FIRST_IN_LIST_FROM_YAML_LONGFILE':\n",
    "        checkers=checkers[0]\n",
    "    elif settings['CHECKERS_INVOLVED'] != 'THOSE_IN_LIST_FROM_YAML_LONGFILE':\n",
    "        print(f\"Unknown option `{settings['CHECKERS_INVOLVED']}` for the setting `CHECKERS_INVOLVED`\")\n",
    "        return\n",
    "    if 'TALight' in checkers:\n",
    "        if len(settings['RTAL_URLS']) == 0:\n",
    "            print(\"Invalid Settings File: `CHECKERS_INVOLVED`=`{settings['CHECKERS_INVOLVED']}` is in contraddiction with `RTAL_URLS` set to the empty list\")\n",
    "            return\n",
    "        answer_dict = { val[1]:val[0] for val in long_answer_dict.values() }\n",
    "        alias_dict = { val[1]:key for key, val in long_answer_dict.items() }\n",
    "        rtalargs_dict = task_dict\n",
    "        rtalargs_dict['input_data_assigned'] = input_data_assigned\n",
    "        rtalargs_dict['alias_dict'] = alias_dict\n",
    "        rtalargs_dict['answer_dict'] = answer_dict\n",
    "        URL_NAME = settings['RTAL_URLS'][0]\n",
    "        URL = settings[URL_NAME]\n",
    "        TAL_feedback_dict = rtal_connect(URL, TALight_problem_name, 'check', rtalargs_dict=rtalargs_dict, output_files_local_folder='output_files_TALight', rtaltoken=RTAL_PRIVATE_ACCESS_TOKEN)\n",
    "        monitor_what_submitted_to_rtald(TALight_problem_name,rtalargs_dict)\n",
    "        print(f\"Here is what the TALight checker thinks (and has stored on the server `{URL_NAME}`) about your submission:\")\n",
    "        if TAL_feedback_dict != None:\n",
    "            display(TAL_feedback_dict['feedback_string'])\n",
    "        elif 'signal_when_no_feedback_has_been_produced' in setting['RTAL_INTERFACE_VERBOSITY']:\n",
    "            print(\"The Feedback received from the server is EMPTY\")\n",
    "        for next_URL_NAME in settings['RTAL_URLS'][1:]:\n",
    "            URL = settings[next_URL_NAME]\n",
    "            print(f\"Next we pose the very same request also to the server `{next_URL_NAME}`:\")\n",
    "            next_TAL_feedback_dict = rtal_connect(URL, TALight_problem_name, 'check', rtalargs_dict=rtalargs_dict, output_files_local_folder='output_files_TALight', rtaltoken=RTAL_PRIVATE_ACCESS_TOKEN)\n",
    "            print(f\"Here is what the TALight checker thinks (and has stored on the server `{next_URL_NAME}`) about your submission:\")\n",
    "            if next_TAL_feedback_dict != None:\n",
    "                display(next_TAL_feedback_dict['feedback_string'])\n",
    "                check_checkers_agreement(URL_NAME,TAL_feedback_dict,next_TAL_feedback_dict,next_URL_NAME)\n",
    "            if next_TAL_feedback_dict != None:\n",
    "                display(TAL_feedback_dict['feedback_string'])\n",
    "            elif 'signal_when_no_feedback_has_been_produced' in setting['RTAL_INTERFACE_VERBOSITY']:\n",
    "                print(\"The Feedback received from the server is EMPTY\")\n",
    "\n",
    "    if 'embedded_in_notebook' in checkers:\n",
    "        print(\"Here is an independent evaluation (just in case the TALight server is down or unreachable):\")\n",
    "        SEF = std_eval_feedback(task_dict)\n",
    "        KingArthur = verify_submission_problem_specific(SEF, input_data_assigned, long_answer_dict)\n",
    "        embedded_feedback_dict = KingArthur.verify_submission(SEF)\n",
    "        #print(f\"feedback_dict={feedback_dict}\", file=stderr)\n",
    "        display(HTML(embedded_feedback_dict['feedback_string']))\n",
    "        if 'TALight' in checkers and settings['CHECK_AGREEMENT_OF_CHECKERS']:\n",
    "            check_checkers_agreement(URL_NAME,TAL_feedback_dict,embedded_feedback_dict)\n",
    "verify_submission(TALight_problem_name='RO_knapsack',checkers=['TALight', 'embedded_in_notebook'],task_dict={'task': 10, 'pt_tot': 40, 'pt_formato_OK': 0, 'pt_feasibility_OK': 1, 'pt_consistency_OK': 0}, input_data_assigned={'labels':['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'L', 'M', 'N'],'costs':[15, 16, 17, 11, 13, 5, 7, 3, 1, 12, 9, 7],'vals':[50, 52, 54, 40, 45, 17, 18, 7, 8, 42, 30, 22],'Knapsack_Capacity':31,'forced_out':['F', 'E'],'forced_in':['B', 'I'],'partialDPtable':[],}, long_answer_dict={'opt_sol':(opt_sol10,'opt_sol10'),})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8924a051",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hide_input": true,
    "init_cell": true,
    "tags": [
     "noexport"
    ]
   },
   "outputs": [],
   "source": [
    "loader_main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02be0492",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hide_input": true,
    "init_cell": true,
    "tags": [
     "noexport"
    ]
   },
   "source": [
    "<h1>_______________________________________________________________________________________ </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdac524",
   "metadata": {
    "deletable": false,
    "editable": false,
    "hide_input": true,
    "init_cell": true,
    "tags": [
     "run_start",
     "noexport"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from urllib.request import urlopen\n",
    "import nbformat\n",
    "from traitlets.config import Config\n",
    "from nbconvert import HTMLExporter\n",
    "from nbconvert import RSTExporter, NotebookExporter\n",
    "from IPython.display import Image, HTML, Javascript\n",
    "from datetime import datetime\n",
    "\n",
    "import shutil\n",
    "last ='preview_last/' #path of last submission\n",
    "old = 'preview_old/' #path of prevous submissions\n",
    "\n",
    "#hide alert buttons\n",
    "def hide_w(w):\n",
    "    for e in w:\n",
    "        e.layout.visibility = 'hidden'\n",
    "\n",
    "#show alert buttons\n",
    "def show_w(w):\n",
    "    for e in w:\n",
    "        e.layout.visibility = 'visible'\n",
    "\n",
    "def are_you_sure():\n",
    "    show_w(alert_buttons)\n",
    "\n",
    "label = widgets.Label(value=\"Un tuo elaborato era gi stato precedentemente prodotto. Vuoi rimpiazzarlo con il presente?\")\n",
    "yes = widgets.Button(description=\"Si\", tooltip=\"Produce il nuovo elaborato. Esso diviene la versione attuale prendendo il posto del precedente\")\n",
    "no = widgets.Button(description=\"No\")\n",
    "\n",
    "alert_buttons = [label,yes,no]\n",
    "hide_w(alert_buttons)\n",
    "\n",
    "#move all files from last to old and save the new preview\n",
    "def yes_clicked(b):\n",
    "    files = os.listdir(last)\n",
    "    for f in files:\n",
    "        if f == '.ipynb_checkpoints':\n",
    "            continue\n",
    "        if not os.path.isdir('./preview_old'):\n",
    "            os.mkdir(old)\n",
    "        shutil.move(last+f, old)\n",
    "    display(Javascript('IPython.notebook.save_checkpoint();'))\n",
    "    now = datetime.now() # current date and time\n",
    "    date_time = now.strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "    hname = 'Esercizio_1.ipynb'[:-6] + '_' + date_time\n",
    "    ! jupyter nbconvert Esercizio_1.ipynb --to html_embed --output=$hname --output-dir=./preview_last/ --TagRemovePreprocessor.enabled=True --TagRemovePreprocessor.remove_cell_tags=\"['noexport']\"\n",
    "    display(Javascript('window.open(\"./preview_last/' + hname + '.html\")'))\n",
    "    hide_w(alert_buttons)\n",
    "def no_clicked(b):\n",
    "    hide_w(alert_buttons)\n",
    "\n",
    "\n",
    "def generate_preview_HTML(_):\n",
    "    if not os.path.isdir('./preview_last'):\n",
    "        os.mkdir(last)\n",
    "    directory= os.listdir(last)\n",
    "    if len(directory) <1:\n",
    "        display(Javascript('IPython.notebook.save_checkpoint();'))\n",
    "        now = datetime.now() # current date and time\n",
    "        date_time = now.strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "        hname = 'Esercizio_1.ipynb'[:-6] + '_' + date_time\n",
    "        ! jupyter nbconvert Esercizio_1.ipynb --TagRemovePreprocessor.enabled=True --TagRemovePreprocessor.remove_cell_tags=\"['noexport']\" --to html_embed --output=$hname --output-dir=./preview_last/\n",
    "        display(Javascript('window.open(\"./preview_last/' + hname + '.html\")'))\n",
    "    else:\n",
    "        are_you_sure()\n",
    "\n",
    "\n",
    "button = widgets.Button(description=\"Salva & Esporta\", tooltip=\"Esporta il foglio Jupyter in HTML nella cartella preview\")\n",
    "output = widgets.Output()\n",
    "button.on_click(generate_preview_HTML)\n",
    "yes.on_click(yes_clicked)\n",
    "no.on_click(no_clicked)\n",
    "h_box1 = widgets.HBox([button])\n",
    "h_box2 = widgets.HBox([label,yes,no])\n",
    "display(widgets.VBox([h_box1,h_box2]))\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
